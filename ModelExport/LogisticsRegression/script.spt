========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
/// 20161013

object colsOperate {

  def testFile(_f : String) = new java.io.File(_f).isFile

  def readFile(_f : String) = scala.io.Source.fromFile(_f).getLines

  def readFile(_f : String, _codec : String) = scala.io.Source.fromFile(_f)(_codec).getLines

  def isFileExist(_f : String) = (new java.io.File(_f).isFile) || (new java.io.File(_f).isDirectory)

  def fileWriter(_f : String) = new java.io.PrintWriter(_f)

  // example : 1.txt => 1.rst or 1.rst => 1-rst.rst
  def getRstName(_f : String) = {
    val idx = _f.indices.map(i => if(_f(i) == '.') i else -1).filter(_ >= 0)
    if(_f.substring(idx.last).equals(".rst"))
      _f.substring(0,idx.last) + "-rst.rst"
    else 
      _f.substring(0,idx.last) + ".rst"
  }

  // split line by given _separator
  def splitLine( _line : String, _separator : Char ) : Array[String] = _line.split(_separator)

  // get the max cols' number
  def getMaxColNum(_f : String, _separator : Char = '\t') : Int = {
    if(!testFile(_f)) {
      System.err.println(_f + "is not a File to calculate the cols max num")
      System.exit(1)
    }
    readFile(_f).map(splitLine(_,_separator).size).max
  }

  // range to Array[Int]
  // format of range : 
  //    1
  //    1,3
  //    1:3
  //    1:3,7:9
  //    1,3,7:9
  //     -1
  def extractRange(_range : String, _f : String = "", _separator : Char = '\t') : Array[Int] = {
    val splits = _range.trim.split(',')
    val sigleIdxSize = splits.filter(_.contains(':')).size
    val range = sigleIdxSize match {
      case 0 => splits.map(_.toInt)
      case _ => {
        val sigleIdx = splits.filter(!_.contains(':')).map(_.toInt)
        val multiIdxs = splits.filter(_.contains(':')).map{
          line => 
            val Array(start,end) = line.split(':')
            var sInt = start.toInt
            var eInt = end.toInt
            if( sInt < 0 && eInt < 0 ){
              val maxColNum = getMaxColNum(_f,_separator)
              sInt += maxColNum + 1 
              eInt += maxColNum + 1
            }
            else if(sInt < 0) sInt += getMaxColNum(_f,_separator) + 1
            else if(eInt < 0) eInt += getMaxColNum(_f,_separator) + 1
            sInt to eInt
        }.flatten
        (sigleIdx.toList ::: multiIdxs.toList toArray).sorted.distinct
      }
    }
    range
  }

  def delCols2File( 
    _in : String, 
    _range : String = "1", 
    _isDel : Boolean = true,  
    _separator : Char = '\t', 
    _out : String = ""
    ) : Unit = {
    if(!testFile(_in)) {
      System.err.println(_in + " not a File!")
      // return
      exit(1)
    }
    val iter = readFile(_in)
    val range = extractRange(_range, _in, _separator).map(_-1)
    val pIter = iter.map{
      line =>
        val splits = splitLine(line,_separator)
        _isDel match {
          case true => {splits.indices.filterNot(range.filter( i => i < splits.size).contains).map(splits).toArray}
          case false => range.filter( i => i < splits.size).map( i => splits(i) )
        }
    }
    val printer = _out match {
      case "" => fileWriter(getRstName(_in))
      case _ => fileWriter(_out)
    }
    while(pIter.hasNext) printer.write(pIter.next.mkString(_separator.toString) + "\n")
    printer.flush
    printer.close
  }
  def delCols( 
    _in : String, 
    _range : String = "1", 
    _isDel : Boolean = true,  
    _separator : Char = '\t'
    ) : Iterator[Array[String]] = {
    if(!testFile(_in)) {
      System.err.println(_in + " not a File!")
      // return
      exit(1)
    }
    val iter = readFile(_in)
    val range = extractRange(_range, _in, _separator).map(_-1)
    val pIter = iter.map{
      line =>
        val splits = splitLine(line,_separator)
        _isDel match {
          case true => {splits.indices.filterNot(range.filter( i => i < splits.size).contains).map(splits).toArray}
          case false => range.filter( i => i < splits.size).map( i => splits(i) )
        }
    }
    pIter
  }
  
  // add the _value to the before _idx
  def addCols(
    _in : String, 
    _value : String,
    _idx : Int,
    _separator : Char = '\t', 
    _out : String = "") : Unit = {
    if(!testFile(_in)) {
      System.err.println(_in + " not a File!")
      // return
      exit(1)
    }
    val iter = readFile(_in)
    val pIter = iter.map{
      line =>
        val splits = line.split(_separator)
        val idx = (_idx > splits.size + 1) match {
          case true => splits.size + 1
          case false => _idx
        }
        (splits.take(idx) :+ _value) ++ splits.takeRight(splits.size - idx)
    }
    val printer = _out match {
      case "" => fileWriter(getRstName(_in))
      case _ => fileWriter(_out)
    }
    while(pIter.hasNext) printer.write(pIter.next.mkString(_separator.toString) + "\n")
    printer.flush
    printer.close
  }

}

object readLibSVM {

  /// single line libsvm to Sparse Vector Format 
  def parseLibSVMRecord(line: String): (Double, Array[Int], Array[Double]) = {
    val items = line.split(' ')
    val label = items.head.toDouble
    val (indices, values) = items.tail.filter(_.nonEmpty).map { item =>
      val indexAndValue = item.split(':')
      val index = indexAndValue(0).toInt - 1 // Convert 1-based indices to 0-based.
      val value = indexAndValue(1).toDouble
      (index, value)
    }.unzip

    // check if indices are one-based and in ascending order
    var previous = -1
    var i = 0
    val indicesLength = indices.length
    while (i < indicesLength) {
      val current = indices(i)
      require(current > previous, s"indices should be one-based and in ascending order;"
        + " found current=$current, previous=$previous; line=\"$line\"")
      previous = current
      i += 1
    }
    (label, indices.toArray, values.toArray)
  }

  // get the libsvm dimision(the max indices)
  def computeNumFeatures(_arr : Array[(Double, Array[Int], Array[Double])]): Int = {
    _arr.map { case (label, indices, values) =>
      indices.lastOption.getOrElse(0)
    }.reduce(math.max) + 1
  }
  def computeNumFeatures(_iter : Iterator[(Double, Array[Int], Array[Double])]): Int = {
    _iter.map { case (label, indices, values) =>
      indices.lastOption.getOrElse(0)
    }.reduce(math.max) + 1
  }

  def parseLibSVMFile( _path: String ): Array[(Double, Array[Int], Array[Double])] = {
    scala.io.Source.fromFile(_path).getLines
      .map(_.trim)
      .filter(line => !(line.isEmpty || line.startsWith("#")))
      .map(parseLibSVMRecord)
      .toArray
  }

  def libsvm2Data( _path : String = "", numFeatures: Int = -1) : Array[(Double,Array[Double])] = {
    val parsed = parseLibSVMFile(_path)
    // Determine number of features.
    val d = if (numFeatures > 0) {
      numFeatures
    } else {
      computeNumFeatures(parsed)
    }
    parsed.map { case (label, indices, values) =>
      val datas = new Array[Double](d)
      indices.indices.map( i => datas(indices(i)) = values(i) )
      label.toDouble -> datas
    }
  }  

  def readParsed( _file : String ) : Iterator[(Double, Array[Int], Array[Double])] = {
    scala.io.Source.fromFile(_file).getLines
      .map(_.trim)
      .filter(line => !(line.isEmpty || line.startsWith("#")))
      .map(parseLibSVMRecord)
  }
  def read( _file : String = "", numFeatures : Int = -1 ) : Iterator[(Double,Array[Double])] = {
    val parsed = readParsed(_file)
    val d = if (numFeatures > 0) {
      numFeatures
    } else {
      computeNumFeatures(readParsed(_file))
    }
    parsed.map { case (label, indices, values) =>
      val datas = new Array[Double](d)
      indices.indices.map( i => datas(indices(i)) = values(i) )
      label.toDouble -> datas
    }
  }

}

object generateLibSVM {

  // one vector in Array[Double] type convert to libsvm, split by space 
  // features'indices start from 1
  def vec2libsvm( _vec : Array[Double] ) : String = {
    _vec
      .zipWithIndex
      .filterNot(_._1 == 0)
      .map{ 
        case( value, idx ) => 
        (idx + 1).toString + ":" + value.toString
      }
      .mkString(" ")
  }
  // generate LabeledPoint in libsvm format by data String 
  def vec2libsvm( _label : Int, _vec : Array[Double] ) : String = _label.toString + " " + vec2libsvm(_vec)

  // vector by DenseData convert to Array[String] by libsvm format
  def data2libsvm( _arr : Array[Array[Double]] ) : Array[String] = _arr.map(vec2libsvm)
  // convert data strings to LabeledPoint in libsvm, get an Array 
  def data2libsvm( _label : Int, _arr : Array[Array[Double]] ) : Array[String] = _arr.map( x => vec2libsvm(_label,x) )
  // vector by DenseData convert to Array[String] by libsvm format
  def data2libsvm( _arr : Iterator[Array[Double]] ) : Iterator[String] = _arr.map(vec2libsvm)
  // convert data strings to LabeledPoint in libsvm, get an Array 
  def data2libsvm( _label : Int, _arr : Iterator[Array[Double]] ) : Iterator[String] = _arr.map(x => vec2libsvm(_label,x))

  def write( _file : String, _label : Int, _arr : Array[Array[Double]] ) : Unit = {
    val w = new java.io.PrintWriter(_file)
    val content = data2libsvm(_label,_arr).mkString("\n")
    w.write(content + "\n")
    w.flush
    w.close
  }
  def write( _file : String, _label : Int, _arr : Iterator[Array[Double]] ) : Unit = {
    val w = new java.io.PrintWriter(_file)
    val contentIter = data2libsvm(_label,_arr)
    while(contentIter.hasNext) w.write(contentIter.next + "\n")
    w.flush
    w.close
  }
}

object feature_map {
  import readLibSVM._
  import generateLibSVM._
  val path = "D:\\Docs\\Works_And_Jobs\\Sina\\Betn_code\\NaiveBayesModelExport\\NaiveBayes\\data"
  val file = path + "\\feature.map"
  val iter = colsOperate.readFile(file)
  val data = colsOperate.readFile(file).filterNot{
    line =>
      line.startsWith("#") || line.startsWith("@")
  }
  
  val map = data.map{
    line =>
      val splits = line.split('\t')
      // if(splits.size > 1)
      splits(0).toInt -> splits(1).toInt
  }.toMap
  
  def loadFeatureMap( _path : String ) : Map[Int,Int] = {
    colsOperate.readFile(file).filterNot{
      line =>
        line.startsWith("#") || line.startsWith("@")
    }.map{
      line =>
        val splits = line.split('\t')
        // if(splits.size > 1)
        splits(0).toInt -> (splits(1) match {
          case "_other_" => 1
          case _ => splits(1).toInt
        })
    }.toMap
  }
}

object loadConfig {
  def strConf2Pair(_str : String) : (String,String) = {
    val splits = _str.split('=')
    splits.head -> splits.last
  }
  def findProcess(_arrConf : Array[String]) : (Array[String],Array[String]) = {
    _arrConf.splitAt(
      _arrConf.indexOf(
        _arrConf.find(_.contains("FEATURE_PROCESSES")).get))
  }
  def readProcess(_arrProcesses : Array[String]) : Map[String,String] = {
      _arrProcesses.tail.init
      .map(_.trim)
      .mkString
      .split(',')
      .map(strConf2Pair)
      .toMap
  }
  def readPaths(_arrPaths : Array[String]) : Map[String,String] = {
    _arrPaths
      .filter(_.contains("="))
      .map(strConf2Pair)
      .toMap
  }
  def loadConf(_path : String) : Map[String,String] = {
    val arrConf = colsOperate.readFile(_path).toArray.filterNot(_.startsWith("#"))
    val (arrPaths,arrProcesses) = findProcess(arrConf)
    val mapPaths = readPaths(arrPaths)
    val mapProcesses = readProcess(arrProcesses)
    mapPaths ++ mapProcesses
  }
}

  import colsOperate._
  import readLibSVM._
  import generateLibSVM._
  import feature_map._
  import loadConfig._
// }

========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
/// 20161014

  import colsOperate._
  import readLibSVM._
  import generateLibSVM._
  import feature_map._
  import loadConfig._
  def processByConfig( _conf : Map[String,String] ) : Unit = {
    delCols()
    addCols()
    readLibSVM()
    write()
    computeNumFeatures()
    getMaxColNum()
    feature_map
  }
// }

object feature_map {
  import readLibSVM._
  import generateLibSVM._
  val path = "D:\\Docs\\Works_And_Jobs\\Sina\\Betn_code\\NaiveBayesModelExport\\NaiveBayes\\data"
  val file = path + "\\feature.map"
  val iter = colsOperate.readFile(file)
  val data = colsOperate.readFile(file).filterNot{
    line =>
      line.startsWith("#") || line.startsWith("@")
  }
  
  val map = data.map{
    line =>
      val splits = line.split('\t')
      // if(splits.size > 1)
      splits(0).toInt -> splits(1).toInt
  }.toMap
  
  def loadFeatureMap( _path : String ) : Map[Int,Int] = {
    colsOperate.readFile(_path).filterNot{
      line =>
        line.startsWith("#") || line.startsWith("@")
    }.map{
      line =>
        val splits = line.split('\t')
        // if(splits.size > 1)
        splits(0).toInt -> (splits(1) match {
          case "_other_" => 1
          case _ => splits(1).toInt
        })
    }.toMap
  }
  def loadFeatureMap( _path : String, _flag : Int ) : Map[String,Map[Int,Int]] = {
    colsOperate.readFile(_path,"UTF-8")
      .toArray
      .mkString("\n")
      .split('#')
      .filterNot( x => x.size < 3 || x.startsWith("@") )
      .map{
        feature_line =>
          val splits = feature_line.split('\n').filter(_.size > 1)
          (splits.size > 0) match {
            case true => {
              val headLine = splits.head.split('\t')
              val features = splits.tail.map{
                feature_line => 
                  val feature_content = feature_line.split('\t')
                  feature_content.head.toInt -> (feature_content(1) match {
                    case "_other_" => 1
                    case _ => feature_content(1).toInt
                  } /// feature_content(1) match 
                )/// feature_content.head -> feature_content(1)
              }.toMap/// splits.tail.map 
              headLine.head -> features
            }/// splits.size > 0 match : case true
            case false => "" -> new Array[(Int,Int)](0).toMap
          }/// splits.size > 0 match 
      }.filter(_._1.size > 0)/// filterNot.map 
      .toMap
  }


========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
/// 20161020

case class DataConf(
  _idx : Int,
  _name : String,
  _category : String,
  _operation : String = "")

def loadDataConf( _path : String, _codec : String = "UTF-8" ) : Map[String,DataConf] = {
  colsOperate.readFile(_path,_codec)
    .toArray
    .filterNot(x => x.startsWith("@") ||
      x.startsWith("#") ||
      x.size < 1)
    .map(_.split('@'))
    .filterNot(_.size < 3)
    .map{
      splits => 
        splits.size match {
          case 3 => new DataConf(splits(0).toInt,splits(1),splits(2))
          case 4 => new DataConf(splits(0).toInt,splits(1),splits(2),splits(3))
          case _ => {
            println("error when read data.conf,format not match!!" + 
              s"${splits.size}")
            new DataConf(-1,"","")
          }/// match case 
        }/// match 
    }/// map 
    .map{
      case(dcf) => dcf._name -> dcf 
    }.toMap
}

case class FeatureConf(
  _name : String,
  _category : String,
  _operation : String = "")

case class FeatureContent(
  _idx : Int,
  _subIdx : Int,
  _default : Int = 0,
  _info : String = "")

def loadFeatureMap( _path : String, _codec : String = "UTF-8" ) : Map[String,Map[Int,Int]] = {
    colsOperate.readFile(_path,_codec)
      .toArray
      .filterNot( x =>  x.startsWith("@") ||x.split('\t').size < 2 )
      .mkString("\n")
      .split('#')
      .map(_.split('\n'))
      .filterNot(_.size < 2)
      .map{
        featureAndContent =>
          val featureSplits = featureAndContent.head.split('\t')
          val contentSplits = featureAndContent.tail.map(_.split('\t'))
          val featureConf = featureSplits.size match {
            case 2 => new FeatureConf(featureSplits(0),featureSplits(1))
            case 3 => new FeatureConf(featureSplits(0),featureSplits(1),featureSplits(2))
            case _ => {
              println("error when read feature.map,format not match!!" + 
              s"error with featureConf.size ${featureSplits.size}")
            }/// match case _
          }/// match 
          val featureContent = contentSplits.map{
            featureContentLine =>
              featureContentLine.size match {
                case 3 => new FeatureContent(
                  featureContentLine(0).toInt,
                  featureContentLine(1).toInt,
                  featureContentLine(2).toInt)
                case 4 => new FeatureContent(
                  featureContentLine(0).toInt,
                  featureContentLine(1).toInt,
                  featureContentLine(2).toInt,
                  featureContentLine(3))
                case _ => {
                  println("error when read feature.map,format not match!!" + 
                  s"error with featureContent.size : ${featureSplits.size}")
                  new FeatureContent(-1,-1)
                }
              }
          }
      }
      .map{
        feature_line =>
          val splits = feature_line.split('\n').filter(_.size > 1)
          (splits.size > 0) match {
            case true => {
              val headLine = splits.head.split('\t')
              val features = splits.tail.map{
                feature_line => 
                  val feature_content = feature_line.split('\t')
                  feature_content.head.toInt -> (feature_content(1) match {
                    case "_other_" => 1
                    case _ => feature_content(1).toInt
                  } /// feature_content(1) match 
                )/// feature_content.head -> feature_content(1)
              }.toMap/// splits.tail.map 
              headLine.head -> features
            }/// splits.size > 0 match : case true
            case false => "" -> new Array[(Int,Int)](0).toMap
          }/// splits.size > 0 match 
      }.filter(_._1.size > 0)/// filterNot.map 
      .toMap
  }