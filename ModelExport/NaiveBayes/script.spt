========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
/// 20161010
if (args.length < 5) {
    System.err.println("Usage: sample <input_data> <output_prefix> <NRfeatures> <fields.special> <model_in>*")
    System.exit(1)
}

========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
/// 20161011
if (args.length < 5) {
    System.err.println("Usage: sample <input_data> <output_prefix> <NRfeatures> <fields.special> <model_in>*")
    System.exit(1)
}

def testFile(_f : String) = new java.io.File(_f).isFile

def readFile(_f : String) = scala.io.Source.fromFile(_f).getLines

def isFileExist(_f : String) = (new java.io.File(_f).isFile) || (new java.io.File(_f).isDirectory)

def fileWriter(_f : String) = new java.io.PrintWriter(_f)

// example : 1.txt => 1.rst or 1.rst => 1-rst.rst
def getRstName(_f : String) = {
  val idx = _f.indices.map(i => if(_f(i) == '.') i else -1).filter(_ >= 0)
  if(_f.substring(idx.last).equals(".rst"))
    _f.substring(0,idx.last) + "-rst.rst"
  else 
    _f.substring(0,idx.last) + ".rst"
}

// split line by given _separator
def splitLine( _line : String, _separator : Char ) : Array[String] = _line.split(_separator)

// get the max cols' number
def getMaxColNum(_f : String, _separator : Char = '\t') : Int = {
  if(testFile(_f)) {
    System.err.println(_f + "is not a File to calculate the cols max num")
    System.exit(1)
  }
  readFile(_f).map(splitLine(_,_separator).size).max
}

// range to Array[Int]
// format of range : 
//    1
//    1,3
//    1:3
//    1:3,7:9
//    1,3,7:9
//     -1
def extractRange(_range : String, _f : String = "", _separator : Char = '\t') : Array[Int] = {
  val splits = _range.trim.split(',')
  val sigleIdxSize = splits.filter(_.contains(':')).size
  val range = sigleIdxSize match {
    case 0 => splits.map(_.toInt)
    case _ => {
      val sigleIdx = splits.filter(!_.contains(':')).map(_.toInt)
      val multiIdxs = splits.filter(_.contains(':')).map{
        line => 
          val Array(start,end) = line.split(':')
          var sInt = start.toInt
          var eInt = end.toInt
          if( sInt < 0 && eInt < 0 ){
            val maxColNum = getMaxColNum(_f,_separator)
            sInt += maxColNum + 1 
            eInt += maxColNum + 1
          }
          else if(sInt < 0) sInt += getMaxColNum(_f,_separator) + 1
          else if(eInt < 0) eInt += getMaxColNum(_f,_separator) + 1
          sInt to eInt
      }.flatten
      (sigleIdx.toList ::: multiIdxs.toList toArray).sorted.distinct
    }
  }
  range
}

def delCols( 
  _in : String, 
  _range : String = "1", 
  _isDel : Boolean = true,  
  _separator : Char = '\t', 
  _out : String = ""
  ) : Unit = {
  if(!testFile(_in)) {
    System.err.println(_in + " not a File!")
    // return
    System.exit(1)
  }
  val iter = readFile(_in)
  val range = extractRange(_range, _in, _separator).map(_-1)
  val pIter = iter.map{
    line =>
      val splits = splitLine(line,_separator)
      _isDel match {
        case true => {splits.indices.filterNot(range.filter( i < splits.size).contains).map(splits).toArray}
        case false => range.filter( i < splits.size).map( i => splits(i) )
      }
  }
  val printer = _out match {
    case "" => fileWriter(getRstName(_in))
    case _ => fileWriter(_out)
  }
  while(pIter.hasNext) printer.write(pIter.next.mkString(_separator.toString) + "\n")
  printer.flush
  printer.close
}

// add the _value to the before _idx
def addCols(
  _in : String, 
  _value : String,
  _idx : Int,
  _separator : Char = '\t', 
  _out : String = "") : Unit = {
  if(!testFile(_in)) {
    System.err.println(_in + " not a File!")
    // return
    System.exit(1)
  }
  val iter = readFile(_in)
  val pIter = iter.map{
    line =>
      val splits = line.split(_separator)
      val idx = (_idx > splits.size + 1) match {
        case true => splits.size + 1
        case false => _idx
      }
      (splits.take(idx) :+ _value) ++ splits.takeRight(splits.size - idx)
  }
  val printer = _out match {
    case "" => fileWriter(getRstName(_in))
    case _ => fileWriter(_out)
  }
  while(pIter.hasNext) printer.write(pIter.next.mkString(_separator.toString) + "\n")
  printer.flush
  printer.close
}



/// single line libsvm to Sparse Vector Format 
def parseLibSVMRecord(line: String): (Double, Array[Int], Array[Double]) = {
  val items = line.split(' ')
  val label = items.head.toDouble
  val (indices, values) = items.tail.filter(_.nonEmpty).map { item =>
    val indexAndValue = item.split(':')
    val index = indexAndValue(0).toInt - 1 // Convert 1-based indices to 0-based.
    val value = indexAndValue(1).toDouble
    (index, value)
  }.unzip

  // check if indices are one-based and in ascending order
  var previous = -1
  var i = 0
  val indicesLength = indices.length
  while (i < indicesLength) {
    val current = indices(i)
    require(current > previous, s"indices should be one-based and in ascending order;"
      + " found current=$current, previous=$previous; line=\"$line\"")
    previous = current
    i += 1
  }
  (label, indices.toArray, values.toArray)
}
// get the libsvm dimision(the max indices)
def computeNumFeatures(_arr : Array[(Double, Array[Int], Array[Double])]): Int = {
  _arr.map { case (label, indices, values) =>
    indices.lastOption.getOrElse(0)
  }.reduce(math.max) + 1
}
def parseLibSVMFile( path: String ): Array[(Double, Array[Int], Array[Double])] = {
  scala.io.Source.fromFile(path).getLines
    .map(_.trim)
    .filter(line => !(line.isEmpty || line.startsWith("#")))
    .map(parseLibSVMRecord)
    .toArray
}
case class WeiLabeledPoint(
  label : Double,
  features : org.apache.spark.mllib.linalg.Vector)
def loadLibSVMFile( path: String, numFeatures: Int = -1): Array[WeiLabeledPoint] = {
  val parsed = parseLibSVMFile(path)

  // Determine number of features.
  val d = if (numFeatures > 0) {
    numFeatures
  } else {
    computeNumFeatures(parsed)
  }

  parsed.map { case (label, indices, values) =>
    new WeiLabeledPoint(label.toDouble,org.apache.spark.mllib.linalg.Vectors.sparse(d, indices, values))
  }
}

========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
/// 20161012

/// single line libsvm to Sparse Vector Format 
def parseLibSVMRecord(line: String): (Double, Array[Int], Array[Double]) = {
  val items = line.split(' ')
  val label = items.head.toDouble
  val (indices, values) = items.tail.filter(_.nonEmpty).map { item =>
    val indexAndValue = item.split(':')
    val index = indexAndValue(0).toInt - 1 // Convert 1-based indices to 0-based.
    val value = indexAndValue(1).toDouble
    (index, value)
  }.unzip

  // check if indices are one-based and in ascending order
  var previous = -1
  var i = 0
  val indicesLength = indices.length
  while (i < indicesLength) {
    val current = indices(i)
    require(current > previous, s"indices should be one-based and in ascending order;"
      + " found current=$current, previous=$previous; line=\"$line\"")
    previous = current
    i += 1
  }
  (label, indices.toArray, values.toArray)
}

// get the libsvm dimision(the max indices)
def computeNumFeatures(_arr : Array[(Double, Array[Int], Array[Double])]): Int = {
  _arr.map { case (label, indices, values) =>
    indices.lastOption.getOrElse(0)
  }.reduce(math.max) + 1
}

def parseLibSVMFile( _path: String ): Array[(Double, Array[Int], Array[Double])] = {
  scala.io.Source.fromFile(_path).getLines
    .map(_.trim)
    .filter(line => !(line.isEmpty || line.startsWith("#")))
    .map(parseLibSVMRecord)
    .toArray
}

def libsvm2Data( _path : String = "", numFeatures: Int = -1) : Array[(Double,Array[Double])] = {
  val parsed = parseLibSVMFile(_path)

  // Determine number of features.
  val d = if (numFeatures > 0) {
    numFeatures
  } else {
    computeNumFeatures(parsed)
  }

  parsed.map { case (label, indices, values) =>
    val datas = new Array[Double](d)
    indices.indices.map( i => datas(indices(i)) = values(i) )
    label.toDouble -> datas
  }
}

// one vector in Array[Double] type convert to libsvm, split by space 
// features'indices start from 1
def vec2libsvm( _vec : Array[Double] ) : String = {
  _vec
    .zipWithIndex
    .filterNot(_._1 == 0)
    .map{ 
      case( value, idx ) => 
      (idx + 1).toString + ":" + value.toString
    }
    .mkString(" ")
}

// generate LabeledPoint in libsvm format by data String 
def vec2libsvm( _label : Int, _vec : Array[Double] ) : String = _label.toString + " " + vec2libsvm(_vec)

// vector by DenseData convert to Array[String] by libsvm format
def data2libsvm( _arr : Array[Array[Double]] ) : Array[String] = _arr.map(vec2libsvm)
// convert data strings to LabeledPoint in libsvm, get an Array 
def data2libsvm( _label : Int, _arr : Array[Array[Double]] ) = _arr.map( x => vec2libsvm(_label,x) )

========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
/// 20161013

object colsOperate{

  def testFile(_f : String) = new java.io.File(_f).isFile

  def readFile(_f : String) = scala.io.Source.fromFile(_f).getLines

  def isFileExist(_f : String) = (new java.io.File(_f).isFile) || (new java.io.File(_f).isDirectory)

  def fileWriter(_f : String) = new java.io.PrintWriter(_f)

  // example : 1.txt => 1.rst or 1.rst => 1-rst.rst
  def getRstName(_f : String) = {
    val idx = _f.indices.map(i => if(_f(i) == '.') i else -1).filter(_ >= 0)
    if(_f.substring(idx.last).equals(".rst"))
      _f.substring(0,idx.last) + "-rst.rst"
    else 
      _f.substring(0,idx.last) + ".rst"
  }

  // split line by given _separator
  def splitLine( _line : String, _separator : Char ) : Array[String] = _line.split(_separator)

  // get the max cols' number
  def getMaxColNum(_f : String, _separator : Char = '\t') : Int = {
    if(!testFile(_f)) {
      System.err.println(_f + "is not a File to calculate the cols max num")
      System.exit(1)
    }
    readFile(_f).map(splitLine(_,_separator).size).max
  }

  // range to Array[Int]
  // format of range : 
  //    1
  //    1,3
  //    1:3
  //    1:3,7:9
  //    1,3,7:9
  //     -1
  def extractRange(_range : String, _f : String = "", _separator : Char = '\t') : Array[Int] = {
    val splits = _range.trim.split(',')
    val sigleIdxSize = splits.filter(_.contains(':')).size
    val range = sigleIdxSize match {
      case 0 => splits.map(_.toInt)
      case _ => {
        val sigleIdx = splits.filter(!_.contains(':')).map(_.toInt)
        val multiIdxs = splits.filter(_.contains(':')).map{
          line => 
            val Array(start,end) = line.split(':')
            var sInt = start.toInt
            var eInt = end.toInt
            if( sInt < 0 && eInt < 0 ){
              val maxColNum = getMaxColNum(_f,_separator)
              sInt += maxColNum + 1 
              eInt += maxColNum + 1
            }
            else if(sInt < 0) sInt += getMaxColNum(_f,_separator) + 1
            else if(eInt < 0) eInt += getMaxColNum(_f,_separator) + 1
            sInt to eInt
        }.flatten
        (sigleIdx.toList ::: multiIdxs.toList toArray).sorted.distinct
      }
    }
    range
  }

  def delCols2File( 
    _in : String, 
    _range : String = "1", 
    _isDel : Boolean = true,  
    _separator : Char = '\t', 
    _out : String = ""
    ) : Unit = {
    if(!testFile(_in)) {
      System.err.println(_in + " not a File!")
      // return
      System.exit(1)
    }
    val iter = readFile(_in)
    val range = extractRange(_range, _in, _separator).map(_-1)
    val pIter = iter.map{
      line =>
        val splits = splitLine(line,_separator)
        _isDel match {
          case true => {splits.indices.filterNot(range.filter( i => i < splits.size).contains).map(splits).toArray}
          case false => range.filter( i => i < splits.size).map( i => splits(i) )
        }
    }
    val printer = _out match {
      case "" => fileWriter(getRstName(_in))
      case _ => fileWriter(_out)
    }
    while(pIter.hasNext) printer.write(pIter.next.mkString(_separator.toString) + "\n")
    printer.flush
    printer.close
  }
  def delCols( 
    _in : String, 
    _range : String = "1", 
    _isDel : Boolean = true,  
    _separator : Char = '\t'
    ) : Iterator[Array[String]] = {
    if(!testFile(_in)) {
      System.err.println(_in + " not a File!")
      // return
      System.exit(1)
    }
    val iter = readFile(_in)
    val range = extractRange(_range, _in, _separator).map(_-1)
    val pIter = iter.map{
      line =>
        val splits = splitLine(line,_separator)
        _isDel match {
          case true => {splits.indices.filterNot(range.filter( i => i < splits.size).contains).map(splits).toArray}
          case false => range.filter( i => i < splits.size).map( i => splits(i) )
        }
    }
    pIter
  }
  
  // add the _value to the before _idx
  def addCols(
    _in : String, 
    _value : String,
    _idx : Int,
    _separator : Char = '\t', 
    _out : String = "") : Unit = {
    if(!testFile(_in)) {
      System.err.println(_in + " not a File!")
      // return
      System.exit(1)
    }
    val iter = readFile(_in)
    val pIter = iter.map{
      line =>
        val splits = line.split(_separator)
        val idx = (_idx > splits.size + 1) match {
          case true => splits.size + 1
          case false => _idx
        }
        (splits.take(idx) :+ _value) ++ splits.takeRight(splits.size - idx)
    }
    val printer = _out match {
      case "" => fileWriter(getRstName(_in))
      case _ => fileWriter(_out)
    }
    while(pIter.hasNext) printer.write(pIter.next.mkString(_separator.toString) + "\n")
    printer.flush
    printer.close
  }

}

object readLibSVM {

  /// single line libsvm to Sparse Vector Format 
  def parseLibSVMRecord(line: String): (Double, Array[Int], Array[Double]) = {
    val items = line.split(' ')
    val label = items.head.toDouble
    val (indices, values) = items.tail.filter(_.nonEmpty).map { item =>
      val indexAndValue = item.split(':')
      val index = indexAndValue(0).toInt - 1 // Convert 1-based indices to 0-based.
      val value = indexAndValue(1).toDouble
      (index, value)
    }.unzip

    // check if indices are one-based and in ascending order
    var previous = -1
    var i = 0
    val indicesLength = indices.length
    while (i < indicesLength) {
      val current = indices(i)
      require(current > previous, s"indices should be one-based and in ascending order;"
        + " found current=$current, previous=$previous; line=\"$line\"")
      previous = current
      i += 1
    }
    (label, indices.toArray, values.toArray)
  }

  // get the libsvm dimision(the max indices)
  def computeNumFeatures(_arr : Array[(Double, Array[Int], Array[Double])]): Int = {
    _arr.map { case (label, indices, values) =>
      indices.lastOption.getOrElse(0)
    }.reduce(math.max) + 1
  }
  def computeNumFeatures(_iter : Iterator[(Double, Array[Int], Array[Double])]): Int = {
    _iter.map { case (label, indices, values) =>
      indices.lastOption.getOrElse(0)
    }.reduce(math.max) + 1
  }

  def parseLibSVMFile( _path: String ): Array[(Double, Array[Int], Array[Double])] = {
    scala.io.Source.fromFile(_path).getLines
      .map(_.trim)
      .filter(line => !(line.isEmpty || line.startsWith("#")))
      .map(parseLibSVMRecord)
      .toArray
  }

  def libsvm2Data( _path : String = "", numFeatures: Int = -1) : Array[(Double,Array[Double])] = {
    val parsed = parseLibSVMFile(_path)
    // Determine number of features.
    val d = if (numFeatures > 0) {
      numFeatures
    } else {
      computeNumFeatures(parsed)
    }
    parsed.map { case (label, indices, values) =>
      val datas = new Array[Double](d)
      indices.indices.map( i => datas(indices(i)) = values(i) )
      label.toDouble -> datas
    }
  }  

  def readParsed( _file : String ) : Iterator[(Double, Array[Int], Array[Double])] = {
    scala.io.Source.fromFile(_file).getLines
      .map(_.trim)
      .filter(line => !(line.isEmpty || line.startsWith("#")))
      .map(parseLibSVMRecord)
  }
  def read( _file : String = "", numFeatures : Int = -1 ) : Iterator[(Double,Array[Double])] = {
    val parsed = readParsed(_file)
    val d = if (numFeatures > 0) {
      numFeatures
    } else {
      computeNumFeatures(readParsed(_file))
    }
    parsed.map { case (label, indices, values) =>
      val datas = new Array[Double](d)
      indices.indices.map( i => datas(indices(i)) = values(i) )
      label.toDouble -> datas
    }
  }

}

object generateLibSVM /*extends Any*/{

  // one vector in Array[Double] type convert to libsvm, split by space 
  // features'indices start from 1
  def vec2libsvm( _vec : Array[Double] ) : String = {
    _vec
      .zipWithIndex
      .filterNot(_._1 == 0)
      .map{ 
        case( value, idx ) => 
        (idx + 1).toString + ":" + value.toString
      }
      .mkString(" ")
  }
  // generate LabeledPoint in libsvm format by data String 
  def vec2libsvm( _label : Int, _vec : Array[Double] ) : String = _label.toString + " " + vec2libsvm(_vec)

  // vector by DenseData convert to Array[String] by libsvm format
  def data2libsvm( _arr : Array[Array[Double]] ) : Array[String] = _arr.map(vec2libsvm)
  // convert data strings to LabeledPoint in libsvm, get an Array 
  def data2libsvm( _label : Int, _arr : Array[Array[Double]] ) : Array[String] = _arr.map( x => vec2libsvm(_label,x) )
  // vector by DenseData convert to Array[String] by libsvm format
  def data2libsvm( _arr : Iterator[Array[Double]] ) : Iterator[String] = _arr.map(vec2libsvm)
  // convert data strings to LabeledPoint in libsvm, get an Array 
  def data2libsvm( _label : Int, _arr : Iterator[Array[Double]] ) : Iterator[String] = _arr.map(x => vec2libsvm(_label,x))

  def write( _file : String, _label : Int, _arr : Array[Array[Double]] ) : Unit = {
    val w = new java.io.PrintWriter(_file)
    val content = data2libsvm(_label,_arr).mkString("\n")
    w.write(content + "\n")
    w.flush
    w.close
  }
  def write( _file : String, _label : Int, _arr : Iterator[Array[Double]] ) : Unit = {
    val w = new java.io.PrintWriter(_file)
    val contentIter = data2libsvm(_label,_arr)
    while(contentIter.hasNext) w.write(contentIter.next + "\n")
    w.flush
    w.close
  }
}

object feature_map {
  import readLibSVM._
  import generateLibSVM._
  val path = "D:\\Docs\\Works_And_Jobs\\Sina\\Betn_code\\NaiveBayesModelExport\\NaiveBayes\\data"
  val file = path + "\\feature.map"
  val iter = colsOperate.readFile(file)
  val data = colsOperate.readFile(file).filterNot{
    line =>
      line.startsWith("#") || line.startsWith("@")
  }
  
  val map = data.map{
    line =>
      val splits = line.split('\t')
      // if(splits.size > 1)
      splits(0).toInt -> splits(1).toInt
  }.toMap
  
  def loadFeatureMap( _path : String ) : Map[Int,Int] = {
    colsOperate.readFile(file).filterNot{
      line =>
        line.startsWith("#") || line.startsWith("@")
    }.map{
      line =>
        val splits = line.split('\t')
        // if(splits.size > 1)
        splits(0).toInt -> (splits(1) match {
          case "_other_" => 1
          case _ => splits(1).toInt
        })
    }.toMap
  }
}

object loadConfig {
  def strConf2Pair(_str : String) : (String,String) = {
    val splits = _str.split('=')
    splits.head -> splits.last
  }
  def findProcess(_arrConf : Array[String]) : (Array[String],Array[String]) = {
    _arrConf.splitAt(
      _arrConf.indexOf(
        _arrConf.find(_.contains("FEATURE_PROCESSES")).get))
  }
  def readProcess(_arrProcesses : Array[String]) : Map[String,String] = {
      _arrProcesses.tail.init
      .map(_.trim)
      .mkString
      .split(',')
      .map(strConf2Pair)
      .toMap
  }
  def readPaths(_arrPaths : Array[String]) : Map[String,String] = {
    _arrPaths
      .filter(_.contains("="))
      .map(strConf2Pair)
      .toMap
  }
  def loadConf(_path : String) : Map[String,String] = {
    val arrConf = colsOperate.readFile(_path).toArray.filterNot(_.startsWith("#"))
    val (arrPaths,arrProcesses) = findProcess(arrConf)
    val mapPaths = readPaths(arrPaths)
    val mapProcesses = readProcess(arrProcesses)
    mapPaths ++ mapProcesses
  }
}

  import colsOperate._
  import readLibSVM._
  import generateLibSVM._
  import feature_map._
  import loadConfig._
// }

========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
/// 20161014

  import colsOperate._
  import readLibSVM._
  import generateLibSVM._
  import feature_map._
  import loadConfig._
  def processByConfig( _conf : Map[String,String] ) : Unit = {
    delCols()
    addCols()
    readLibSVM()
    write()
    computeNumFeatures()
    getMaxColNum()
    feature_map
  }
// }

object feature_map {
  import readLibSVM._
  import generateLibSVM._
  val path = "D:\\Docs\\Works_And_Jobs\\Sina\\Betn_code\\NaiveBayesModelExport\\NaiveBayes\\data"
  val file = path + "\\feature.map"
  val iter = colsOperate.readFile(file)
  val data = colsOperate.readFile(file).filterNot{
    line =>
      line.startsWith("#") || line.startsWith("@")
  }
  
  val map = data.map{
    line =>
      val splits = line.split('\t')
      // if(splits.size > 1)
      splits(0).toInt -> splits(1).toInt
  }.toMap
  
  def loadFeatureMap( _path : String ) : Map[Int,Int] = {
    colsOperate.readFile(file).filterNot{
      line =>
        line.startsWith("#") || line.startsWith("@")
    }.map{
      line =>
        val splits = line.split('\t')
        // if(splits.size > 1)
        splits(0).toInt -> (splits(1) match {
          case "_other_" => 1
          case _ => splits(1).toInt
        })
    }.toMap
  }
  def loadFeatureMap( _path : String, _flag : Int ) : Map[String,Map[Int,Int]] = {
    colsOperate.readFile(file)
      .toArray
      .mkString("\n")
      .split('#')
      .filterNot( x => x.size < 3 || x.startsWith("@") )
      .map{
        feature_line =>
          val splits = feature_line.split('\n').filter(_.size > 1)
          (splits.size > 0) match {
            case true => {
              val headLine = splits.head.split('\t')
              val features = splits.tail.map{
                feature_line => 
                  val feature_content = feature_line.split('\t')
                  feature_content.head.toInt -> (feature_content(1) match {
                    case "_other_" => 1
                    case _ => feature_content(1).toInt
                  } /// feature_content(1) match 
                )/// feature_content.head -> feature_content(1)
              }.toMap/// splits.tail.map 
              headLine.head -> features
            }/// splits.size > 0 match : case true
            case false => "" -> new Array[(Int,Int)](0).toMap
          }/// splits.size > 0 match 
      }.filter(_._1.size > 0)/// filterNot.map 
      .toMap
  }



========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
/// 20161030


object ConfUtil extends Serializable {

  case class DataConf(
    _idx : Int,
    _name : String,
    _category : String = "",
    _operation : String = "") extends Serializable

  case class FeatureConf(
    _name : String,
    _category : String,
    _operation : String = "") extends Serializable

  case class FeatureContent(
    _idx : Int,
    _subIdx : Int,
    _default : Int = 0,
    _info : String = "") extends Serializable

  type DataConfType = Map[String,ConfUtil.DataConf]

  type FeatureMapType = Map[String,(ConfUtil.FeatureConf,Array[ConfUtil.FeatureContent])]

}

object ConfFileHelper extends Serializable  {

  def testFile(_f : String) = new java.io.File(_f).isFile

  def readFile(_f : String) = scala.io.Source.fromFile(_f).getLines

  def readFile(_f : String, _codec : String) = scala.io.Source.fromFile(_f)(_codec).getLines

}

object RDDConfParser extends Serializable {
  val seperator = '\t'
  @transient
  def loadDataConf( _path : String, _codec : String = "UTF-8" ) : ConfUtil.DataConfType = {
    val contents = ConfFileHelper.readFile(_path,_codec)
      .toArray
      .filterNot(x => x.startsWith("@") ||
        x.startsWith("#") ||
        x.size < 1)
      .map(_.split('@'))
      .filterNot(_.size < 2)
      .map{
        splits => 
          splits.size match {
            case 2 => new ConfUtil.DataConf(splits(0).toInt,splits(1))
            case 3 => new ConfUtil.DataConf(splits(0).toInt,splits(1),splits(2))
            case 4 => new ConfUtil.DataConf(splits(0).toInt,splits(1),splits(2),splits(3))
            case _ => {
              println("error when read data.conf,format not match!!" + 
                s"${splits.size}")
              new ConfUtil.DataConf(-1,"","")
            }
          }
      } 
      .map{
        case(dcf) => dcf._name -> dcf 
      }.toMap
      contents
  }
  @transient
  def loadFeatureMap( _path : String, 
    _codec : String = "UTF-8" ) : ConfUtil.FeatureMapType = {
    ConfFileHelper.readFile(_path,_codec)
      .toArray
      .filterNot( x =>  x.startsWith("@") ||
        (x.split(seperator).size < 2 && x.split(seperator).size < 2))
      .mkString("\n")
      .split('#')
      .map(_.split('\n'))
      .filterNot(_.size < 2)
      .map{
        featureAndContent =>
          val featureSplits = featureAndContent.head.split(seperator).size match {
            case 1 => featureAndContent.head.split(seperator)
            case _ => featureAndContent.head.split(seperator)
          }
          val contentSplits = featureAndContent.tail.map(_.split(seperator).size).max  match {
            case 1 => featureAndContent.tail.map(_.split(seperator))
            case _ => featureAndContent.tail.map(_.split(seperator))
          }
          val featureConf = featureSplits.size match {
            case 2 => new ConfUtil.FeatureConf(featureSplits(0),featureSplits(1))
            case 3 => new ConfUtil.FeatureConf(featureSplits(0),featureSplits(1),featureSplits(2))
            case _ => {
              println("error when read feature.map,format not match!!" + 
              s"error with featureConf.size ${featureSplits.size}")
              new ConfUtil.FeatureConf("","")
            }/// match case _
          }/// match 
          val featureContent = contentSplits.map{
            featureContentLine =>
              // println(featureContentLine.head)
              featureContentLine.size match {
                case 2 => contentSplits(1).size.equals("_other_") match {
                  case true => new ConfUtil.FeatureContent(
                    featureContentLine(0).toInt,
                    featureContentLine(1).toInt,0,
                    "")
                  case false => new ConfUtil.FeatureContent(
                    featureContentLine(0).toInt,
                    featureContentLine.size -1, 0,
                    featureContentLine(1))
                }
                case 3 => new ConfUtil.FeatureContent(
                  featureContentLine(0).toInt,
                  featureContentLine(1).toInt,
                  featureContentLine(2).toInt)
                case 4 => new ConfUtil.FeatureContent(
                  featureContentLine(0).toInt,
                  featureContentLine(1).toInt,
                  featureContentLine(2).toInt,
                  featureContentLine(3))
                case _ => {
                  println("error when read feature.map,format not match!!" + 
                  s"error with featureContentLine.size : ${featureContentLine.size}")
                  new ConfUtil.FeatureContent(-1,-1)
                }/// match case
              }/// match 
          }/// contentSplits.map 
          // .toMap
          featureConf -> featureContent
      }/// filterNot.map 
      .map( x => x._1._name -> x )
      .toMap
  }
  @transient
  def getColsID( _dcc : ConfUtil.DataConfType, 
    _fmc : ConfUtil.FeatureMapType,
    _isLabeled : Boolean = true, /// @Deprecated 
    _defaultLabel : Int = 0 /// @Deprecated 
    ) : Array[(String,Int)] = {
    val featureNames = _fmc.toArray.map(_._1)
    val rstIdxs = featureNames.map( x => _dcc.get(x).get._name -> _dcc.get(x).get._idx)
    val _isHasLabel = _dcc.get("label") match {
      case None => false
      case Some(_) => true
    }
    (_isLabeled && _isHasLabel) match {
      case true => ((_dcc.get("label").get._name -> _dcc.get("label").get._idx) +: rstIdxs ).sortBy{x => x._2}
      case false => rstIdxs.sortBy{_._2}
    }
  }
  @transient
  def readMaxColNum( _fmc : ConfUtil.FeatureMapType ) : Int = _fmc.map(_._2._2.map(_._idx)).flatten.max 
}

val path = "D:\\Docs\\Works_And_Jobs\\Sina\\Betn_code\\ModelExport\\NaiveBayes\\data"
val nFile = path + "\\normal_sample.txt"
val aFile = path + "\\abnormal_sample.txt"
val dataConfFile = path + "\\data.conf"
val featureConfFile = path + "\\feature.conf"

val dcc = RDDConfParser.loadDataConf(dataConfFile)
val fmc = RDDConfParser.loadFeatureMap(featureConfFile)
val idxs = dcc.map(_._2._idx).toArray.sorted
val oprators = dcc.toArray.sortBy(_._2._idx).map(_._2._operation)
val maxSize = fmc.toArray.sortBy(x => dcc.get(x._1).get._idx).map(_._2._2.size)
def delCols(_str : String) : Array[String] = {
  val splits = _str.split('\t')
  val size = splits.size
  if(size <= idxs.last-1) {
    println(s"error of input by col size is ${size}, for without label less than ${idxs.last-1}")
    return Array("")
  }
  val rst = idxs.map( i => splits(i-1) )
  rst
}
def dataMappor(_arr : Array[String]) : Array[Int] = {
  val rst = new Array[Array[Int]](idxs.size).indices.map( i => new Array[Int](maxSize(i))).toArray
  _arr.indices.map{
    i => 
      val subIdx = oprators(i) match {
        case "div1" => _arr(i).toInt/1
        case _ => scala.math.log(_arr(i).toInt).toInt
      }
      (subIdx >= 0 && subIdx < maxSize(i)) match {
        case true => rst(i)(subIdx) = 1
        case false => rst(i)(maxSize(i)-1) = 1
      }
  }
  rst.flatten
}
val aData = sc.textFile(aFile).map( x => dataMappor(delCols(x)) )
val nData = sc.textFile(nFile).map( x => dataMappor(delCols(x)) )
val aRstLibsvm = aData.map(x => RDDgenerateLibSVM.vec2libsvm(1,x.map(_.toDouble)))
val nRstLibsvm = nData.map(x => RDDgenerateLibSVM.vec2libsvm(0,x.map(_.toDouble)))

val aRstLP = aData.map{
  x => 
    val pair = x.zipWithIndex.filterNot(_._1 == 0).unzip
    val features = org.apache.spark.mllib.linalg.Vectors.sparse(x.size,pair._2,pair._1.map(_.toDouble))
    new org.apache.spark.mllib.regression.LabeledPoint(1,features)
}
val nRstLP = nData.map{
  x => 
    val pair = x.zipWithIndex.filterNot(_._1 == 0).unzip
    val features = org.apache.spark.mllib.linalg.Vectors.sparse(x.size,pair._2,pair._1.map(_.toDouble))
    new org.apache.spark.mllib.regression.LabeledPoint(0,features)
}



