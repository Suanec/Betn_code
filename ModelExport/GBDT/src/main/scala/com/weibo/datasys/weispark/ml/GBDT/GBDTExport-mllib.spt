import java.io.File
import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.linalg.DenseVector
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}
import org.apache.spark.mllib.evaluation.MulticlassMetrics


// import org.apache.spark.mllib.util.MLUtil.
val path = "D:\\ScalaSpace\\data\\financialCardMatch\\matchData"
val trainFile = path + "\\train1.csv"
val trianRate = Array(0.8,0.2)
val rst = new Array[(String,Double)](4)
rst(0) = "LRacc : " -> 0d
rst(1) = "DTacc : " -> 0d
rst(2) = "RFacc : " -> 0d
rst(3) = "GBDTacc : " -> 0d

def isNum( _c : Char ) : Boolean = _c >= '0' && _c <= '9'


  def csvFile(_path : String, _rateZero : Int = 1) : RDD[LabeledPoint] = {
    val rawData = scala.io.Source.fromFile(trainFile)("utf-8").getLines.drop(1).toArray
    val data = sc.parallelize(rawData).flatMap{
      line =>
        val splits = line.split(',')
        val label = splits.last.toDouble.toInt
        val features = new DenseVector(splits.init.map(_.toDouble))
        label match {
          case 0 => Array.fill[LabeledPoint](_rateZero)(new LabeledPoint(label,features))
          case 1 => Array.fill[LabeledPoint](1)(new LabeledPoint(label,features))
          case _ => Array.fill(0)(new LabeledPoint(label,features))
        }
    }  
    data
  }
  def convertNum(_str : String) : Double = {
    val rst = _str.head match {
      case '-' => "-0" + _str.tail 
      case '.' => "0" + _str
      case _ => _str
    }
    rst.toDouble
  }

val data = csvFile(trainFile)

import org.apache.spark.mllib.tree.GradientBoostedTrees
import org.apache.spark.mllib.tree.configuration.BoostingStrategy
import org.apache.spark.mllib.tree.model.GradientBoostedTreesModel

val splits = data.randomSplit(trianRate)
val (trainingData, testData) = (splits(0), splits(1))

// Train a GradientBoostedTrees model.
// The defaultParams for Classification use LogLoss by default.
val boostingStrategy = BoostingStrategy.defaultParams("Classification")
boostingStrategy.numIterations = 3 // Note: Use more iterations in practice.
boostingStrategy.treeStrategy.numClasses = 2
boostingStrategy.treeStrategy.maxDepth = 5
// Empty categoricalFeaturesInfo indicates all features are continuous.
boostingStrategy.treeStrategy.categoricalFeaturesInfo = Map[Int, Int]()

val model = GradientBoostedTrees.train(trainingData, boostingStrategy)

// Evaluate model on test instances and compute test error
val labelAndPreds = testData.map { point =>
  val prediction = model.predict(point.features)
  (point.label, prediction)
}
rst(3) = "GBDTacc" -> labelAndPreds.filter(r => r._1 == r._2).count.toDouble / testData.count()



rst.mkString("\n")


import org.apache.spark.mllib.tree.model.DecisionTreeModel

class TreeModelMeta(var trees: Array[DecisionTreeModel], var treeWeights: Array[Double])  extends Serializable {
  var version:String = "1.0";
  var loadedClassName:String = "";
  var storageType:String = "JSON"; //JSON, XML

  var algo:String = "Classification"; //Classification, Regression
  var treeAlgo:String = "Regression";
  var combiningStrategy:String = "Sum"; //Vote, Average, Sum

  var nClasses:Int = 2
  var nFeatures:Int = 0
  var nTrees:Int = 0
  var nLeaves:Int = 0
  var nNodes:Int = 0

  var impurity:String = "Gini";
  var maxDepth:Int = 10;
  var maxBins:Int = 10;

  var treesXmlString:String = "";

  //case class Metadata(
  //   algo: String,
  //  treeAlgo: String,
  //  combiningStrategy: String,
  //  treeWeights: Array[Double])

  override def toString():String={
      val sb=new StringBuilder()
      sb.append("##class=" + this.loadedClassName + "\n")
      sb.append("##version=" + this.version + "\n")
      sb.append("\n")
      sb.append("##algo=" + this.algo + "\n")
      sb.append("##treeAlgo=" + this.treeAlgo + "\n")
      sb.append("##combiningStrategy=" + this.combiningStrategy + "\n")
      sb.append("##treeWeights=" + this.treeWeights.mkString(",") + "\n")
      sb.append("\n")
      sb.append("##nClasses=" + this.nClasses + "\n")
      sb.append("##nFeatures=" + this.nFeatures + "\n")
      sb.append("##nTrees=" + this.nTrees + "\n")
      sb.append("##nLeaves=" + this.nLeaves + "\n")
      sb.append("##nNodes=" + this.nNodes + "\n")
      sb.toString()
      
  }
  def toJsonString():String={
      val sb=new StringBuilder()
      sb.append("algo "+this.algo+"\n")
      sb.append("nClasses "+this.nClasses+"\n")
      sb.append("\n")
      sb.append("nFeatures "+this.nFeatures+"\n")
      sb.append("w\n")
      sb.toString()
  }
  def toMetaString():String={
      val sb=new StringBuilder()
      sb.append("##MART\n")
      sb.append("\n")
      sb.append("##class=" + this.loadedClassName + "\n")
      sb.append("##version=" + this.version + "\n")
      sb.append("\n")
      sb.append("##algo=" + this.algo + "\n")
      sb.append("##treeAlgo=" + this.treeAlgo + "\n")
      sb.append("##combiningStrategy=" + this.combiningStrategy + "\n")
      sb.append("##treeWeights=" + this.treeWeights.mkString(",")  + "\n")
      sb.append("\n")
      sb.append("##nClasses=" + this.nClasses + "\n")
      sb.append("##nFeatures=" + this.nFeatures + "\n")
      sb.append("##nTrees=" + this.nTrees + "\n")
      sb.append("##nLeaves=" + this.nLeaves + "\n")
      sb.append("##nNodes=" + this.nNodes + "\n")
      sb.append("\n")
      sb.toString()
  }
  def toTreesString():String={
      val sb=new StringBuilder()
      sb.append("\n")
      sb.toString()
  }

  def updateMetaString(lines:String):Unit = {
      lines.split("\n").foreach(content => {
          System.out.println("content: " + content)
          val keyValue: Array[String] = content.split("=")
          if (keyValue.length == 2) {
              if (keyValue.apply(0).trim.indexOf("##class") == 0) {
                  loadedClassName = keyValue.apply(1).trim
              } else if (keyValue.apply(0).trim.indexOf("##version") == 0) {
                  version = keyValue.apply(1).trim
              } else if (keyValue.apply(0).trim.indexOf("##algo") == 0) {
                  algo = keyValue.apply(1).trim
              } else if (keyValue.apply(0).trim.indexOf("##treeAlgo") == 0) {
                  treeAlgo = keyValue.apply(1).trim
              } else if (keyValue.apply(0).trim.indexOf("##combiningStrategy") == 0) {
                  combiningStrategy = keyValue.apply(1).trim
              } else if (keyValue.apply(0).trim.indexOf("##treeWeights") == 0) {
                  treeWeights = keyValue.apply(1).trim.split(",").map(_.toDouble)
              } else if (keyValue.apply(0).trim.indexOf("##nClasses") == 0) {
                  nClasses = keyValue.apply(1).trim.toInt
              } else if (keyValue.apply(0).trim.indexOf("##nFeatures") == 0) {
                  nFeatures = keyValue.apply(1).trim.toInt
              } else if (keyValue.apply(0).trim.indexOf("##nTrees") == 0) {
                  nTrees = keyValue.apply(1).trim.toInt
              } else if (keyValue.apply(0).trim.indexOf("##nLeaves") == 0) {
                  nLeaves = keyValue.apply(1).trim.toInt
              } else if (keyValue.apply(0).trim.indexOf("##nNodes") == 0) {
                  nNodes = keyValue.apply(1).trim.toInt
              }
          }
      })
  }
}

object TreeModelMeta {

  def fromMetaString(content:String, meta:TreeModelMeta):Unit = {
      val keyValue:Array[String] = content.split("=")
      if (keyValue.length == 2) {
          if (keyValue.apply(0).trim.indexOf("##class") == 0) {
              meta.loadedClassName = keyValue.apply(1).trim
          } else if (keyValue.apply(0).trim.indexOf("##version") == 0) {
              meta.version = keyValue.apply(1).trim
          } else if (keyValue.apply(0).trim.indexOf("##algo") == 0) {
              meta.algo = keyValue.apply(1).trim
          } else if (keyValue.apply(0).trim.indexOf("##treeAlgo") == 0) {
              meta.treeAlgo = keyValue.apply(1).trim
          } else if (keyValue.apply(0).trim.indexOf("##combiningStrategy") == 0) {
              meta.combiningStrategy = keyValue.apply(1).trim
          } else if (keyValue.apply(0).trim.indexOf("##treeWeights") == 0) {
              meta.treeWeights = keyValue.apply(1).trim.asInstanceOf[Array[Double]]
          } else if (keyValue.apply(0).trim.indexOf("##nClasses") == 0) {
              meta.nClasses = keyValue.apply(1).trim.toInt
          } else if (keyValue.apply(0).trim.indexOf("##nFeatures") == 0) {
              meta.nFeatures = keyValue.apply(1).trim.toInt
          } else if (keyValue.apply(0).trim.indexOf("##nTrees") == 0) {
              meta.nTrees = keyValue.apply(1).trim.toInt
          } else if (keyValue.apply(0).trim.indexOf("##nLeaves") == 0) {
              meta.nLeaves = keyValue.apply(1).trim.toInt
          }
      }
  }

  def getTreeAlgoFromMetaString(lines:String):String = {
      var tmpTreeAlgo = "Regression"
      lines.split("\n").foreach(content => {
          val keyValue: Array[String] = content.split("=")
          if (keyValue.length == 2) {
              if (keyValue.apply(0).trim.indexOf("##treeAlgo") == 0) {
                  tmpTreeAlgo = keyValue.apply(1).trim
              }
          }
      })
      tmpTreeAlgo
  }

}


import java.io._

// import com.weibo.datasys.weispark.tree.{RegressionTree, Ensemble}
import org.apache.hadoop.fs.{FSDataInputStream, Path}
import org.apache.spark.util.Utils
import org.apache.spark.SparkContext
import org.apache.spark.mllib.classification.LogisticRegressionModel
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.GeneralizedLinearModel
import org.apache.spark.mllib.tree.configuration.Algo
import org.apache.spark.mllib.tree.configuration.Algo
import org.apache.spark.mllib.tree.configuration.Algo.Algo
import org.apache.spark.mllib.tree.model._
import org.apache.spark.mllib.util.Loader
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SQLContext
import org.json4s._
import org.json4s.JsonDSL._
import org.json4s.jackson.JsonMethods._

import scala.collection.mutable
import scala.io.Source

object TreeModelUtil {
  /**
   * save the model to local file system
   */
  def saveRandomForestTreeModelToFile(sc: SparkContext, model: RandomForestModel, path: String) {
      // method 1:
      //model.save(sc, path)

      // method 2:
      import java.io._
      val outWriter = new BufferedWriter(new FileWriter(new File(path)))

      val meta = saveRandomForestModelToMeta(model)

      outWriter.append(meta.toMetaString())
      outWriter.append("\n");
      outWriter.append(meta.treesXmlString)

      outWriter.close()
  }

  def saveRandomForestModelToMeta(model: RandomForestModel):TreeModelMeta = {
      val meta = new TreeModelMeta(model.trees, Array.fill(model.trees.length)(1.0))

      meta.version = "1.0"
      meta.loadedClassName = "org.apache.spark.mllib.tree.model.RandomForestModel"

      meta.algo = model.algo.toString //"Classification"
      meta.treeAlgo = model.trees(0).algo.toString
      //meta.combiningStrategy = model.combiningStrategy.toString
      meta.combiningStrategy = if (model.algo == Algo.Classification) "Vote" else "Average"

      meta.nTrees = model.numTrees
      meta.nNodes = model.totalNumNodes

      //meta.treesXmlString = treesXmlStringFromDecisionTreeModelArray(meta.trees)
      //meta.treesXmlString = treesJsonStringFromDecisionTreeModelArray(meta.trees)
      var (treesXmlString, leavesNum) = treesJsonStringFromDecisionTreeModelArray(meta.trees, 0)
      meta.treesXmlString = treesXmlString
      meta.nLeaves = leavesNum

      meta
  }

  def saveDecisionTreeModelToFile(sc: SparkContext, model: DecisionTreeModel, path: String) {
      // method 1:
      //model.save(sc, path)

      // method 2:
      import java.io._
      val outWriter = new BufferedWriter(new FileWriter(new File(path)))

      val meta = saveDecisionTreeModelToMeta(model)

      outWriter.append(meta.toMetaString())
      outWriter.append("\n");
      outWriter.append(meta.treesXmlString)

      outWriter.close()
  }

  def saveDecisionTreeModelToMeta(model: DecisionTreeModel):TreeModelMeta = {
      val trees:Array[DecisionTreeModel] = Array.fill(1)(model)
      val meta = new TreeModelMeta(trees, Array.fill(trees.length)(1.0))

      meta.version = "1.0"
      meta.loadedClassName = "org.apache.spark.mllib.tree.DecisionTreeModel"

      meta.algo = model.algo.toString //"Classification"
      meta.nNodes = model.numNodes
      meta.maxDepth = model.depth

      //meta.treesXmlString = treesXmlStringFromDecisionTreeModelArray(meta.trees)
      //meta.treesXmlString = treesJsonStringFromDecisionTreeModelArray(meta.trees)
      var (treesXmlString, leavesNum) = treesJsonStringFromDecisionTreeModelArray(meta.trees, 0)
      meta.treesXmlString = treesXmlString
      meta.nLeaves = leavesNum

      meta
  }

  def saveGradientBoostedTreesModelToFile(sc: SparkContext, model: GradientBoostedTreesModel, path: String, firstNodeId:Int): Unit ={
      // method 1:
      //model.save(sc, path)

      // method 2:
      import java.io._
      val outWriter = new BufferedWriter(new FileWriter(new File(path)))

      val meta = saveGradientBoostedTreesModelToMeta(model, firstNodeId)

      outWriter.append(meta.toMetaString())
      outWriter.append("\n");
      outWriter.append(meta.treesXmlString)

      outWriter.close()
  }

  def saveGradientBoostedTreesModelToMeta(model: GradientBoostedTreesModel, firstNodeId:Int):TreeModelMeta = {
      val meta = new TreeModelMeta(model.trees, model.treeWeights)

      meta.version = "1.0"
      meta.loadedClassName = "org.apache.spark.mllib.tree.model.GradientBoostedTreesModel"

      meta.algo = model.algo.toString //"Classification"
      meta.treeAlgo = model.trees(0).algo.toString //"Regression"
      //meta.combiningStrategy = "Sum"
      meta.nTrees = model.numTrees
      meta.nNodes = model.totalNumNodes

      //meta.treesXmlString = treesXmlStringFromDecisionTreeModelArray(meta.trees)
      var (treesXmlString, leavesNum) = treesJsonStringFromDecisionTreeModelArray(meta.trees, firstNodeId)
      meta.treesXmlString = treesXmlString
      meta.nLeaves = leavesNum

      meta.storageType = "JSON"
      //meta.storageType = "XML"

      meta
  }

  def treesJsonStringFromDecisionTreeModelArray (treesArray:Array[DecisionTreeModel], nodeId:Int) : (String, Int) = {
      var treesJsonString: String = ""
      var newNodeId = nodeId
      var i = 0
      for (tree <- treesArray) {
          val (tmpTreesJsonString, tmpLeavesNum ) = treesJsonStringFromDecisionTreeNode(i, tree.topNode, newNodeId)
          treesJsonString += tmpTreesJsonString
          newNodeId = tmpLeavesNum // incr all
          //treesJsonString += "\n"
          i += 1
      }
      //System.out.println("nodeId: " + nodeId + ", newNodeId: " + newNodeId)
      (treesJsonString, newNodeId)
  }

  case class SplitData(
      feature: Int,
      threshold: Double,
      featureType: Int,
      categories: Seq[Double]) //{
    // def toSplit: Split = {
    //   new Split(feature, threshold, FeatureType(featureType), categories.toList)
    // }
  // }

  object SplitData {
    def apply(s: Split): SplitData = {
      SplitData(s.feature, s.threshold, s.featureType.id, s.categories)
    }

    // def apply(r: Row): SplitData = {
    //   SplitData(r.getInt(0), r.getDouble(1), r.getInt(2), r.getAs[Seq[Double]](3))
    // }
  }

  def splitToSplitData(data: Option[Split]): Option[SplitData] = data match {
    case Some(s) => Option(new SplitData(s.feature, s.threshold, s.featureType.id, s.categories))
    case None => None
  }

  def treesJsonStringFromDecisionTreeNode (treeId:Int, node:Node, nodeId:Int) : (String, Int) = {
      var treesJsonString: String = ""
      var newNodeId = nodeId
      implicit val formats = DefaultFormats

      newNodeId += 1

      var jsonStr: JsonAST.JObject = ("treeId" -> treeId) ~
        ("nodeId" -> node.id) ~
        ("predict.predict" -> node.predict.predict) ~
        ("predict.prob" -> node.predict.prob) ~
        ("predict" -> Extraction.decompose(node.predict)) ~
        ("impurity" -> node.impurity) ~
        ("isLeaf" -> node.isLeaf) ~
        ("golbalId" -> newNodeId) ~
        ("split" -> Extraction.decompose(node.split)) ~
        // ("split" -> Extraction.decompose(splitToSplitData(node.split).toArray)) ~
        ("leftNodeId" -> (if(node.leftNode.isEmpty) {-1} else {node.leftNode.get.id})) ~
        ("rightNodeId" -> (if(node.rightNode.isEmpty) {-1} else {node.rightNode.get.id})) ~
        ("infoGain" -> (if(node.stats.isEmpty) {0} else {node.stats.get.gain}))
        ("stats" -> Extraction.decompose(node.stats))
      treesJsonString += compact(render(jsonStr))
      treesJsonString += "\n"

      if (node.leftNode.isDefined && !node.leftNode.isEmpty) {
          val (tmpTreesJsonString, tmpNodeId) = treesJsonStringFromDecisionTreeNode(treeId, node.leftNode.get, newNodeId)
          treesJsonString += tmpTreesJsonString
          newNodeId = tmpNodeId
          //treesJsonString += "\n"
      }

      if (node.rightNode.isDefined && !node.rightNode.isEmpty) {
          val (tmpTreesJsonString, tmpNodeId) = treesJsonStringFromDecisionTreeNode(treeId, node.rightNode.get, newNodeId)
          treesJsonString += tmpTreesJsonString
          newNodeId = tmpNodeId
          //treesJsonString += "\n"
      }

      (treesJsonString, newNodeId)
  }
}

val rstPath = """D:\Docs\Works_And_Jobs\Sina\Betn_code\ModelExport\GBDT\rst\"""
val ofp = (rstPath + "gbdt-" + 
  new java.text.SimpleDateFormat("yyyy-MM-dd-HH").
  format(System.currentTimeMillis) + ".model")
TreeModelUtil.saveGradientBoostedTreesModelToFile(sc,model,ofp,1)
println("""TreeModelUtil.saveGradientBoostedTreesModelToFile(sc,model,ofp,1)""")

========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
========================================================================================
/// 20161121


import scala.reflect.ClassTag

import org.apache.hadoop.fs.Path
import org.json4s._
import org.json4s.jackson.JsonMethods._

import org.apache.spark.ml.linalg.{Vector, Vectors}
import org.apache.spark.ml.param.{Param, Params}
import org.apache.spark.ml.tree.DecisionTreeModelReadWrite.NodeData
import org.apache.spark.ml.util.{DefaultParamsReader, DefaultParamsWriter}
import org.apache.spark.ml.util.DefaultParamsReader.Metadata
import org.apache.spark.mllib.tree.impurity.ImpurityCalculator
import org.apache.spark.mllib.tree.model.{DecisionTreeModel => OldDecisionTreeModel}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{Dataset, SparkSession}
import org.apache.spark.util.collection.OpenHashMap

/**
 * Abstraction for Decision Tree models.
 *
 * TODO: Add support for predicting probabilities and raw predictions  SPARK-3727
 */
trait DecisionTreeModel {

  /** Root of the decision tree */
  def rootNode: Node

  /** Number of nodes in tree, including leaf nodes. */
  def numNodes: Int = {
    1 + rootNode.numDescendants
  }

  /**
   * Depth of the tree.
   * E.g.: Depth 0 means 1 leaf node.  Depth 1 means 1 internal node and 2 leaf nodes.
   */
  lazy val depth: Int = {
    rootNode.subtreeDepth
  }

  /** Summary of the model */
  override def toString: String = {
    // Implementing classes should generally override this method to be more descriptive.
    s"DecisionTreeModel of depth $depth with $numNodes nodes"
  }

  /** Full description of model */
  def toDebugString: String = {
    val header = toString + "\n"
    header + rootNode.subtreeToString(2)
  }

  /**
   * Trace down the tree, and return the largest feature index used in any split.
   *
   * @return  Max feature index used in a split, or -1 if there are no splits (single leaf node).
   */
  def maxSplitFeatureIndex(): Int = rootNode.maxSplitFeatureIndex()

  /** Convert to spark.mllib DecisionTreeModel (losing some information) */
  def toOld: OldDecisionTreeModel
}

/**
 * Abstraction for models which are ensembles of decision trees
 *
 * TODO: Add support for predicting probabilities and raw predictions  SPARK-3727
 *
 * @tparam M  Type of tree model in this ensemble
 */
trait TreeEnsembleModel[M <: DecisionTreeModel] {

  // Note: We use getTrees since subclasses of TreeEnsembleModel will store subclasses of
  //       DecisionTreeModel.

  /** Trees in this ensemble. Warning: These have null parent Estimators. */
  def trees: Array[M]

  /**
   * Number of trees in ensemble
   */
  val getNumTrees: Int = trees.length

  /** Weights for each tree, zippable with [[trees]] */
  def treeWeights: Array[Double]

  /** Weights used by the python wrappers. */
  // Note: An array cannot be returned directly due to serialization problems.
  def javaTreeWeights: Vector = Vectors.dense(treeWeights)

  /** Summary of the model */
  override def toString: String = {
    // Implementing classes should generally override this method to be more descriptive.
    s"TreeEnsembleModel with ${trees.length} trees"
  }

  /** Full description of model */
  def toDebugString: String = {
    val header = toString + "\n"
    header + trees.zip(treeWeights).zipWithIndex.map { case ((tree, weight), treeIndex) =>
      s"  Tree $treeIndex (weight $weight):\n" + tree.rootNode.subtreeToString(4)
    }.fold("")(_ + _)
  }

  /** Total number of nodes, summed over all trees in the ensemble. */
  lazy val totalNumNodes: Int = trees.map(_.numNodes).sum
}

object TreeEnsembleModel {

  /**
   * Given a tree ensemble model, compute the importance of each feature.
   * This generalizes the idea of "Gini" importance to other losses,
   * following the explanation of Gini importance from "Random Forests" documentation
   * by Leo Breiman and Adele Cutler, and following the implementation from scikit-learn.
   *
   * For collections of trees, including boosting and bagging, Hastie et al.
   * propose to use the average of single tree importances across all trees in the ensemble.
   *
   * This feature importance is calculated as follows:
   *  - Average over trees:
   *     - importance(feature j) = sum (over nodes which split on feature j) of the gain,
   *       where gain is scaled by the number of instances passing through node
   *     - Normalize importances for tree to sum to 1.
   *  - Normalize feature importance vector to sum to 1.
   *
   *  References:
   *  - Hastie, Tibshirani, Friedman. "The Elements of Statistical Learning, 2nd Edition." 2001.
   *
   * @param trees  Unweighted collection of trees
   * @param numFeatures  Number of features in model (even if not all are explicitly used by
   *                     the model).
   *                     If -1, then numFeatures is set based on the max feature index in all trees.
   * @return  Feature importance values, of length numFeatures.
   */
  def featureImportances[M <: DecisionTreeModel](trees: Array[M], numFeatures: Int): Vector = {
    val totalImportances = new OpenHashMap[Int, Double]()
    trees.foreach { tree =>
      // Aggregate feature importance vector for this tree
      val importances = new OpenHashMap[Int, Double]()
      computeFeatureImportance(tree.rootNode, importances)
      // Normalize importance vector for this tree, and add it to total.
      // TODO: In the future, also support normalizing by tree.rootNode.impurityStats.count?
      val treeNorm = importances.map(_._2).sum
      if (treeNorm != 0) {
        importances.foreach { case (idx, impt) =>
          val normImpt = impt / treeNorm
          totalImportances.changeValue(idx, normImpt, _ + normImpt)
        }
      }
    }
    // Normalize importances
    normalizeMapValues(totalImportances)
    // Construct vector
    val d = if (numFeatures != -1) {
      numFeatures
    } else {
      // Find max feature index used in trees
      val maxFeatureIndex = trees.map(_.maxSplitFeatureIndex()).max
      maxFeatureIndex + 1
    }
    if (d == 0) {
      assert(totalImportances.size == 0, s"Unknown error in computing feature" +
        s" importance: No splits found, but some non-zero importances.")
    }
    val (indices, values) = totalImportances.iterator.toSeq.sortBy(_._1).unzip
    Vectors.sparse(d, indices.toArray, values.toArray)
  }

  /**
   * Given a Decision Tree model, compute the importance of each feature.
   * This generalizes the idea of "Gini" importance to other losses,
   * following the explanation of Gini importance from "Random Forests" documentation
   * by Leo Breiman and Adele Cutler, and following the implementation from scikit-learn.
   *
   * This feature importance is calculated as follows:
   *  - importance(feature j) = sum (over nodes which split on feature j) of the gain,
   *    where gain is scaled by the number of instances passing through node
   *  - Normalize importances for tree to sum to 1.
   *
   * @param tree  Decision tree to compute importances for.
   * @param numFeatures  Number of features in model (even if not all are explicitly used by
   *                     the model).
   *                     If -1, then numFeatures is set based on the max feature index in all trees.
   * @return  Feature importance values, of length numFeatures.
   */
  def featureImportances[M <: DecisionTreeModel : ClassTag](tree: M, numFeatures: Int): Vector = {
    featureImportances(Array(tree), numFeatures)
  }

  /**
   * Recursive method for computing feature importances for one tree.
   * This walks down the tree, adding to the importance of 1 feature at each node.
   *
   * @param node  Current node in recursion
   * @param importances  Aggregate feature importances, modified by this method
   */
  def computeFeatureImportance(
      node: Node,
      importances: OpenHashMap[Int, Double]): Unit = {
    node match {
      case n: InternalNode =>
        val feature = n.split.featureIndex
        val scaledGain = n.gain * n.impurityStats.count
        importances.changeValue(feature, scaledGain, _ + scaledGain)
        computeFeatureImportance(n.leftChild, importances)
        computeFeatureImportance(n.rightChild, importances)
      case n: LeafNode =>
      // do nothing
    }
  }

  /**
   * Normalize the values of this map to sum to 1, in place.
   * If all values are 0, this method does nothing.
   *
   * @param map  Map with non-negative values.
   */
  def normalizeMapValues(map: OpenHashMap[Int, Double]): Unit = {
    val total = map.map(_._2).sum
    if (total != 0) {
      val keys = map.iterator.map(_._1).toArray
      keys.foreach { key => map.changeValue(key, 0.0, _ / total) }
    }
  }
}

/** Helper classes for tree model persistence */
object DecisionTreeModelReadWrite {

  /**
   * Info for a [[org.apache.spark.ml.tree.Split]]
   *
   * @param featureIndex  Index of feature split on
   * @param leftCategoriesOrThreshold  For categorical feature, set of leftCategories.
   *                                   For continuous feature, threshold.
   * @param numCategories  For categorical feature, number of categories.
   *                       For continuous feature, -1.
   */
  case class SplitData(
      featureIndex: Int,
      leftCategoriesOrThreshold: Array[Double],
      numCategories: Int) {

    def getSplit: Split = {
      if (numCategories != -1) {
        new CategoricalSplit(featureIndex, leftCategoriesOrThreshold, numCategories)
      } else {
        assert(leftCategoriesOrThreshold.length == 1, s"DecisionTree split data expected" +
          s" 1 threshold for ContinuousSplit, but found thresholds: " +
          leftCategoriesOrThreshold.mkString(", "))
        new ContinuousSplit(featureIndex, leftCategoriesOrThreshold(0))
      }
    }
  }

  object SplitData {
    def apply(split: Split): SplitData = split match {
      case s: CategoricalSplit =>
        SplitData(s.featureIndex, s.leftCategories, s.numCategories)
      case s: ContinuousSplit =>
        SplitData(s.featureIndex, Array(s.threshold), -1)
    }
  }

  /**
   * Info for a [[Node]]
   *
   * @param id  Index used for tree reconstruction.  Indices follow a pre-order traversal.
   * @param impurityStats  Stats array.  Impurity type is stored in metadata.
   * @param gain  Gain, or arbitrary value if leaf node.
   * @param leftChild  Left child index, or arbitrary value if leaf node.
   * @param rightChild  Right child index, or arbitrary value if leaf node.
   * @param split  Split info, or arbitrary value if leaf node.
   */
  case class NodeData(
    id: Int,
    prediction: Double,
    impurity: Double,
    impurityStats: Array[Double],
    gain: Double,
    leftChild: Int,
    rightChild: Int,
    split: SplitData)

  object NodeData {
    /**
     * Create [[NodeData]] instances for this node and all children.
     *
     * @param id  Current ID.  IDs are assigned via a pre-order traversal.
     * @return (sequence of nodes in pre-order traversal order, largest ID in subtree)
     *         The nodes are returned in pre-order traversal (root first) so that it is easy to
     *         get the ID of the subtree's root node.
     */
    def build(node: Node, id: Int): (Seq[NodeData], Int) = node match {
      case n: InternalNode =>
        val (leftNodeData, leftIdx) = build(n.leftChild, id + 1)
        val (rightNodeData, rightIdx) = build(n.rightChild, leftIdx + 1)
        val thisNodeData = NodeData(id, n.prediction, n.impurity, n.impurityStats.stats,
          n.gain, leftNodeData.head.id, rightNodeData.head.id, SplitData(n.split))
        (thisNodeData +: (leftNodeData ++ rightNodeData), rightIdx)
      case _: LeafNode =>
        (Seq(NodeData(id, node.prediction, node.impurity, node.impurityStats.stats,
          -1.0, -1, -1, SplitData(-1, Array.empty[Double], -1))),
          id)
    }
  }

  /**
   * Load a decision tree from a file.
   * @return  Root node of reconstructed tree
   */
  def loadTreeNodes(
      path: String,
      metadata: DefaultParamsReader.Metadata,
      sparkSession: SparkSession): Node = {
    import sparkSession.implicits._
    implicit val format = DefaultFormats

    // Get impurity to construct ImpurityCalculator for each node
    val impurityType: String = {
      val impurityJson: JValue = metadata.getParamValue("impurity")
      Param.jsonDecode[String](compact(render(impurityJson)))
    }

    val dataPath = new Path(path, "data").toString
    val data = sparkSession.read.parquet(dataPath).as[NodeData]
    buildTreeFromNodes(data.collect(), impurityType)
  }

  /**
   * Given all data for all nodes in a tree, rebuild the tree.
   * @param data  Unsorted node data
   * @param impurityType  Impurity type for this tree
   * @return Root node of reconstructed tree
   */
  def buildTreeFromNodes(data: Array[NodeData], impurityType: String): Node = {
    // Load all nodes, sorted by ID.
    val nodes = data.sortBy(_.id)
    // Sanity checks; could remove
    assert(nodes.head.id == 0, s"Decision Tree load failed.  Expected smallest node ID to be 0," +
      s" but found ${nodes.head.id}")
    assert(nodes.last.id == nodes.length - 1, s"Decision Tree load failed.  Expected largest" +
      s" node ID to be ${nodes.length - 1}, but found ${nodes.last.id}")
    // We fill `finalNodes` in reverse order.  Since node IDs are assigned via a pre-order
    // traversal, this guarantees that child nodes will be built before parent nodes.
    val finalNodes = new Array[Node](nodes.length)
    nodes.reverseIterator.foreach { case n: NodeData =>
      val impurityStats = ImpurityCalculator.getCalculator(impurityType, n.impurityStats)
      val node = if (n.leftChild != -1) {
        val leftChild = finalNodes(n.leftChild)
        val rightChild = finalNodes(n.rightChild)
        new InternalNode(n.prediction, n.impurity, n.gain, leftChild, rightChild,
          n.split.getSplit, impurityStats)
      } else {
        new LeafNode(n.prediction, n.impurity, impurityStats)
      }
      finalNodes(n.id) = node
    }
    // Return the root node
    finalNodes.head
  }
}

object EnsembleModelReadWrite {

  /**
   * Helper method for saving a tree ensemble to disk.
   *
   * @param instance  Tree ensemble model
   * @param path  Path to which to save the ensemble model.
   * @param extraMetadata  Metadata such as numFeatures, numClasses, numTrees.
   */
  def saveImpl[M <: Params with TreeEnsembleModel[_ <: DecisionTreeModel]](
      instance: M,
      path: String,
      sql: SparkSession,
      extraMetadata: JObject): Unit = {
    DefaultParamsWriter.saveMetadata(instance, path, sql.sparkContext, Some(extraMetadata))
    val treesMetadataWeights: Array[(Int, String, Double)] = instance.trees.zipWithIndex.map {
      case (tree, treeID) =>
        (treeID,
          DefaultParamsWriter.getMetadataToSave(tree.asInstanceOf[Params], sql.sparkContext),
          instance.treeWeights(treeID))
    }
    val treesMetadataPath = new Path(path, "treesMetadata").toString
    sql.createDataFrame(treesMetadataWeights).toDF("treeID", "metadata", "weights")
      .write.parquet(treesMetadataPath)
    val dataPath = new Path(path, "data").toString
    val nodeDataRDD = sql.sparkContext.parallelize(instance.trees.zipWithIndex).flatMap {
      case (tree, treeID) => EnsembleNodeData.build(tree, treeID)
    }
    sql.createDataFrame(nodeDataRDD).write.parquet(dataPath)
  }

  /**
   * Helper method for loading a tree ensemble from disk.
   * This reconstructs all trees, returning the root nodes.
   * @param path  Path given to `saveImpl`
   * @param className  Class name for ensemble model type
   * @param treeClassName  Class name for tree model type in the ensemble
   * @return  (ensemble metadata, array over trees of (tree metadata, root node)),
   *          where the root node is linked with all descendents
   * @see `saveImpl` for how the model was saved
   */
  def loadImpl(
      path: String,
      sql: SparkSession,
      className: String,
      treeClassName: String): (Metadata, Array[(Metadata, Node)], Array[Double]) = {
    import sql.implicits._
    implicit val format = DefaultFormats
    val metadata = DefaultParamsReader.loadMetadata(path, sql.sparkContext, className)

    // Get impurity to construct ImpurityCalculator for each node
    val impurityType: String = {
      val impurityJson: JValue = metadata.getParamValue("impurity")
      Param.jsonDecode[String](compact(render(impurityJson)))
    }

    val treesMetadataPath = new Path(path, "treesMetadata").toString
    val treesMetadataRDD: RDD[(Int, (Metadata, Double))] = sql.read.parquet(treesMetadataPath)
      .select("treeID", "metadata", "weights").as[(Int, String, Double)].rdd.map {
      case (treeID: Int, json: String, weights: Double) =>
        treeID -> (DefaultParamsReader.parseMetadata(json, treeClassName), weights)
    }

    val treesMetadataWeights = treesMetadataRDD.sortByKey().values.collect()
    val treesMetadata = treesMetadataWeights.map(_._1)
    val treesWeights = treesMetadataWeights.map(_._2)

    val dataPath = new Path(path, "data").toString
    val nodeData: Dataset[EnsembleNodeData] =
      sql.read.parquet(dataPath).as[EnsembleNodeData]
    val rootNodesRDD: RDD[(Int, Node)] =
      nodeData.rdd.map(d => (d.treeID, d.nodeData)).groupByKey().map {
        case (treeID: Int, nodeData: Iterable[NodeData]) =>
          treeID -> DecisionTreeModelReadWrite.buildTreeFromNodes(nodeData.toArray, impurityType)
      }
    val rootNodes: Array[Node] = rootNodesRDD.sortByKey().values.collect()
    (metadata, treesMetadata.zip(rootNodes), treesWeights)
  }

  /**
   * Info for one [[Node]] in a tree ensemble
   *
   * @param treeID  Tree index
   * @param nodeData  Data for this node
   */
  case class EnsembleNodeData(
      treeID: Int,
      nodeData: NodeData)

  object EnsembleNodeData {
    /**
     * Create [[EnsembleNodeData]] instances for the given tree.
     *
     * @return Sequence of nodes for this tree
     */
    def build(tree: DecisionTreeModel, treeID: Int): Seq[EnsembleNodeData] = {
      val (nodeData: Seq[NodeData], _) = NodeData.build(tree.rootNode, 0)
      nodeData.map(nd => EnsembleNodeData(treeID, nd))
    }
  }
}

