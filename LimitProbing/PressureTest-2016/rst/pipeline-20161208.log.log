spark-submit --jars /data0/work_space/service/spark-2.0.0-bin-hadoop2.4/jars/hadoop-lzo-0.4.15.jar --master yarn --deploy-mode client --num-executors 100 --driver-memory 7g --executor-cores 4 --executor-memory 7g --class com.weibo.datasys.pipeline.Runner weispark-ml-0.5.0-SNAPSHOT.jar pipeline.xml [5]
Running pipeline: ML training: Resources Limit Probing

16/12/08 22:34:50 ERROR TaskSetManager: Total size of serialized results of 1333 tasks (1024.7 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 1333 tasks (1024.7 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1934)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:983)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:965)
	at com.weibo.datasys.algorithms.Optimization.TronLR.gradient(Tron.scala:107)
	at com.weibo.datasys.algorithms.Optimization.Tron.optimize(Tron.scala:244)
	at org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:313)
	at org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:234)
	at com.weibo.datasys.algorithms.LogisticRegressionWithTron$.run(LogisticRegressionWithTron.scala:29)
	at com.weibo.datasys.pipeline.MLConfig$$anonfun$run$1.apply(MLConfig.scala:35)
	at com.weibo.datasys.pipeline.MLConfig$$anonfun$run$1.apply(MLConfig.scala:29)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at com.weibo.datasys.pipeline.MLConfig.run(MLConfig.scala:29)
	at com.weibo.datasys.pipeline.Framework$$anonfun$runJob$2.apply(Framework.scala:25)
	at com.weibo.datasys.pipeline.Framework$$anonfun$runJob$2.apply(Framework.scala:23)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at com.weibo.datasys.pipeline.Framework$.runJob(Framework.scala:23)
	at com.weibo.datasys.pipeline.Runner$.main(Runner.scala:26)
	at com.weibo.datasys.pipeline.Runner.main(Runner.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
16/12/08 22:34:50 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(15,WrappedArray((9535,5,0,Vector(AccumulableInfo(175335,Some(internal.metrics.executorDeserializeTime),Some(0),None,true,true,None), AccumulableInfo(175336,Some(internal.metrics.executorRunTime),Some(0),None,true,true,None), AccumulableInfo(175337,Some(internal.metrics.resultSize),Some(0),None,true,true,None), AccumulableInfo(175338,Some(internal.metrics.jvmGCTime),Some(4474),None,true,true,None), AccumulableInfo(175339,Some(internal.metrics.resultSerializationTime),Some(0),None,true,true,None), AccumulableInfo(175340,Some(internal.metrics.memoryBytesSpilled),Some(0),None,true,true,None), AccumulableInfo(175341,Some(internal.metrics.diskBytesSpilled),Some(0),None,true,true,None), AccumulableInfo(175342,Some(internal.metrics.peakExecutionMemory),Some(302514176),None,true,true,None), AccumulableInfo(175343,Some(internal.metrics.updatedBlockStatuses),Some(ArrayBuffer()),None,true,true,None), AccumulableInfo(175344,Some(internal.metrics.shuffle.read.remoteBlocksFetched),Some(0),None,true,true,None), AccumulableInfo(175345,Some(internal.metrics.shuffle.read.localBlocksFetched),Some(0),None,true,true,None), AccumulableInfo(175346,Some(internal.metrics.shuffle.read.remoteBytesRead),Some(0),None,true,true,None), AccumulableInfo(175347,Some(internal.metrics.shuffle.read.localBytesRead),Some(0),None,true,true,None), AccumulableInfo(175348,Some(internal.metrics.shuffle.read.fetchWaitTime),Some(0),None,true,true,None), AccumulableInfo(175349,Some(internal.metrics.shuffle.read.recordsRead),Some(0),None,true,true,None), AccumulableInfo(175350,Some(internal.metrics.shuffle.write.bytesWritten),Some(0),None,true,true,None), AccumulableInfo(175351,Some(internal.metrics.shuffle.write.recordsWritten),Some(0),None,true,true,None), AccumulableInfo(175352,Some(internal.metrics.shuffle.write.writeTime),Some(0),None,true,true,None), AccumulableInfo(175353,Some(internal.metrics.input.bytesRead),Some(134217728),None,true,true,None), AccumulableInfo(175354,Some(internal.metrics.input.recordsRead),Some(15095),None,true,true,None), AccumulableInfo(175355,Some(internal.metrics.output.bytesWritten),Some(0),None,true,true,None), AccumulableInfo(175356,Some(internal.metrics.output.recordsWritten),Some(0),None,true,true,None), AccumulableInfo(58469,Some(duration total (min, med, max)),Some(-1),None,true,true,Some(sql)), AccumulableInfo(58462,Some(number of output rows),Some(15095),None,true,true,Some(sql)), AccumulableInfo(58463,Some(number of output rows),Some(15095),None,true,true,Some(sql)), AccumulableInfo(58464,Some(number of output rows),Some(15095),None,true,true,Some(sql)), AccumulableInfo(58465,Some(sort time total (min, med, max)),Some(115),None,true,true,Some(sql)), AccumulableInfo(58466,Some(peak memory total (min, med, max)),Some(302514175),None,true,true,Some(sql)), AccumulableInfo(58467,Some(spill size total (min, med, max)),Some(-1),None,true,true,Some(sql)), AccumulableInfo(58468,Some(number of output rows),Some(2542),None,true,true,Some(sql)))), (9560,5,0,Vector(AccumulableInfo(175335,Some(internal.metrics.executorDeserializeTime),Some(0),None,true,true,None), AccumulableInfo(175336,Some(internal.metrics.executorRunTime),Some(0),None,true,true,None), AccumulableInfo(175337,Some(internal.metrics.resultSize),Some(0),None,true,true,None), AccumulableInfo(175338,Some(internal.metrics.jvmGCTime),Some(4070),None,true,true,None), AccumulableInfo(175339,Some(internal.metrics.resultSerializationTime),Some(0),None,true,true,None), AccumulableInfo(175340,Some(internal.metrics.memoryBytesSpilled),Some(0),None,true,true,None), AccumulableInfo(175341,Some(internal.metrics.diskBytesSpilled),Some(0),None,true,true,None), AccumulableInfo(175342,Some(internal.metrics.peakExecutionMemory),Some(302514176),None,true,true,None), AccumulableInfo(175343,Some(internal.metrics.updatedBlockStatuses),Some(ArrayBuffer()),None,true,true,None), AccumulableInfo(175344,Some(internal.metrics.shuffle.read.remoteBlocksFetched),Some(0),None,true,true,None), AccumulableInfo(175345,Some(internal.metrics.shuffle.read.localBlocksFetched),Some(0),None,true,true,None), AccumulableInfo(175346,Some(internal.metrics.shuffle.read.remoteBytesRead),Some(0),None,true,true,None), AccumulableInfo(175347,Some(internal.metrics.shuffle.read.localBytesRead),Some(0),None,true,true,None), AccumulableInfo(175348,Some(internal.metrics.shuffle.read.fetchWaitTime),Some(0),None,true,true,None), AccumulableInfo(175349,Some(internal.metrics.shuffle.read.recordsRead),Some(0),None,true,true,None), AccumulableInfo(175350,Some(internal.metrics.shuffle.write.bytesWritten),Some(0),None,true,true,None), AccumulableInfo(175351,Some(internal.metrics.shuffle.write.recordsWritten),Some(0),None,true,true,None), AccumulableInfo(175352,Some(internal.metrics.shuffle.write.writeTime),Some(0),None,true,true,None), AccumulableInfo(175353,Some(internal.metrics.input.bytesRead),Some(134217728),None,true,true,None), AccumulableInfo(175354,Some(internal.metrics.input.recordsRead),Some(15095),None,true,true,None), AccumulableInfo(175355,Some(internal.metrics.output.bytesWritten),Some(0),None,true,true,None), AccumulableInfo(175356,Some(internal.metrics.output.recordsWritten),Some(0),None,true,true,None), AccumulableInfo(58469,Some(duration total (min, med, max)),Some(-1),None,true,true,Some(sql)), AccumulableInfo(58462,Some(number of output rows),Some(15095),None,true,true,Some(sql)), AccumulableInfo(58463,Some(number of output rows),Some(15095),None,true,true,Some(sql)), AccumulableInfo(58464,Some(number of output rows),Some(15095),None,true,true,Some(sql)), AccumulableInfo(58465,Some(sort time total (min, med, max)),Some(72),None,true,true,Some(sql)), AccumulableInfo(58466,Some(peak memory total (min, med, max)),Some(302514175),None,true,true,Some(sql)), AccumulableInfo(58467,Some(spill size total (min, med, max)),Some(-1),None,true,true,Some(sql)), AccumulableInfo(58468,Some(number of output rows),Some(1628),None,true,true,Some(sql)))), (9531,5,0,Vector(AccumulableInfo(175335,Some(internal.metrics.executorDeserializeTime),Some(0),None,true,true,None), AccumulableInfo(175336,Some(internal.metrics.executorRunTime),Some(0),None,true,true,None), AccumulableInfo(175337,Some(internal.metrics.resultSize),Some(0),None,true,true,None), AccumulableInfo(175338,Some(internal.metrics.jvmGCTime),Some(4591),None,true,true,None), AccumulableInfo(175339,Some(internal.metrics.resultSerializationTime),Some(0),None,true,true,None), AccumulableInfo(175340,Some(internal.metrics.memoryBytesSpilled),Some(0),None,true,true,None), AccumulableInfo(175341,Some(internal.metrics.diskBytesSpilled),Some(0),None,true,true,None), AccumulableInfo(175342,Some(internal.metrics.peakExecutionMemory),Some(302514176),None,true,true,None), AccumulableInfo(175343,Some(internal.metrics.updatedBlockStatuses),Some(ArrayBuffer()),None,true,true,None), AccumulableInfo(175344,Some(internal.metrics.shuffle.read.remoteBlocksFetched),Some(0),None,true,true,None), AccumulableInfo(175345,Some(internal.metrics.shuffle.read.localBlocksFetched),Some(0),None,true,true,None), AccumulableInfo(175346,Some(internal.metrics.shuffle.read.remoteBytesRead),Some(0),None,true,true,None), AccumulableInfo(175347,Some(internal.metrics.shuffle.read.localBytesRead),Some(0),None,true,true,None), AccumulableInfo(175348,Some(internal.metrics.shuffle.read.fetchWaitTime),Some(0),None,true,true,None), AccumulableInfo(175349,Some(internal.metrics.shuffle.read.recordsRead),Some(0),None,true,true,None), AccumulableInfo(175350,Some(internal.metrics.shuffle.write.bytesWritten),Some(0),None,true,true,None), AccumulableInfo(175351,Some(internal.metrics.shuffle.write.recordsWritten),Some(0),None,true,true,None), AccumulableInfo(175352,Some(internal.metrics.shuffle.write.writeTime),Some(0),None,true,true,None), AccumulableInfo(175353,Some(internal.metrics.input.bytesRead),Some(134217728),None,true,true,None), AccumulableInfo(175354,Some(internal.metrics.input.recordsRead),Some(15095),None,true,true,None), AccumulableInfo(175355,Some(internal.metrics.output.bytesWritten),Some(0),None,true,true,None), AccumulableInfo(175356,Some(internal.metrics.output.recordsWritten),Some(0),None,true,true,None), AccumulableInfo(58469,Some(duration total (min, med, max)),Some(-1),None,true,true,Some(sql)), AccumulableInfo(58462,Some(number of output rows),Some(15095),None,true,true,Some(sql)), AccumulableInfo(58463,Some(number of output rows),Some(15095),None,true,true,Some(sql)), AccumulableInfo(58464,Some(number of output rows),Some(15095),None,true,true,Some(sql)), AccumulableInfo(58465,Some(sort time total (min, med, max)),Some(144),None,true,true,Some(sql)), AccumulableInfo(58466,Some(peak memory total (min, med, max)),Some(302514175),None,true,true,Some(sql)), AccumulableInfo(58467,Some(spill size total (min, med, max)),Some(-1),None,true,true,Some(sql)), AccumulableInfo(58468,Some(number of output rows),Some(3184),None,true,true,Some(sql)))), (9540,5,0,Vector(AccumulableInfo(175335,Some(internal.metrics.executorDeserializeTime),Some(0),None,true,true,None), AccumulableInfo(175336,Some(internal.metrics.executorRunTime),Some(0),None,true,true,None), AccumulableInfo(175337,Some(internal.metrics.resultSize),Some(0),None,true,true,None), AccumulableInfo(175338,Some(internal.metrics.jvmGCTime),Some(4405),None,true,true,None), AccumulableInfo(175339,Some(internal.metrics.resultSerializationTime),Some(0),None,true,true,None), AccumulableInfo(175340,Some(internal.metrics.memoryBytesSpilled),Some(0),None,true,true,None), AccumulableInfo(175341,Some(internal.metrics.diskBytesSpilled),Some(0),None,true,true,None), AccumulableInfo(175342,Some(internal.metrics.peakExecutionMemory),Some(302514176),None,true,true,None), AccumulableInfo(175343,Some(internal.metrics.updatedBlockStatuses),Some(ArrayBuffer()),None,true,true,None), AccumulableInfo(175344,Some(internal.metrics.shuffle.read.remoteBlocksFetched),Some(0),None,true,true,None), AccumulableInfo(175345,Some(internal.metrics.shuffle.read.localBlocksFetched),Some(0),None,true,true,None), AccumulableInfo(175346,Some(internal.metrics.shuffle.read.remoteBytesRead),Some(0),None,true,true,None), AccumulableInfo(175347,Some(internal.metrics.shuffle.read.localBytesRead),Some(0),None,true,true,None), AccumulableInfo(175348,Some(internal.metrics.shuffle.read.fetchWaitTime),Some(0),None,true,true,None), AccumulableInfo(175349,Some(internal.metrics.shuffle.read.recordsRead),Some(0),None,true,true,None), AccumulableInfo(175350,Some(internal.metrics.shuffle.write.bytesWritten),Some(0),None,true,true,None), AccumulableInfo(175351,Some(internal.metrics.shuffle.write.recordsWritten),Some(0),None,true,true,None), AccumulableInfo(175352,Some(internal.metrics.shuffle.write.writeTime),Some(0),None,true,true,None), AccumulableInfo(175353,Some(internal.metrics.input.bytesRead),Some(134217728),None,true,true,None), AccumulableInfo(175354,Some(internal.metrics.input.recordsRead),Some(15095),None,true,true,None), AccumulableInfo(175355,Some(internal.metrics.output.bytesWritten),Some(0),None,true,true,None), AccumulableInfo(175356,Some(internal.metrics.output.recordsWritten),Some(0),None,true,true,None), AccumulableInfo(58469,Some(duration total (min, med, max)),Some(-1),None,true,true,Some(sql)), AccumulableInfo(58462,Some(number of output rows),Some(15095),None,true,true,Some(sql)), AccumulableInfo(58463,Some(number of output rows),Some(15095),None,true,true,Some(sql)), AccumulableInfo(58464,Some(number of output rows),Some(15095),None,true,true,Some(sql)), AccumulableInfo(58465,Some(sort time total (min, med, max)),Some(158),None,true,true,Some(sql)), AccumulableInfo(58466,Some(peak memory total (min, med, max)),Some(302514175),None,true,true,Some(sql)), AccumulableInfo(58467,Some(spill size total (min, med, max)),Some(-1),None,true,true,Some(sql)), AccumulableInfo(58468,Some(number of output rows),Some(3095),None,true,true,Some(sql))))))
16/12/08 22:34:50 ERROR Inbox: Ignoring error
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:152)
	at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:132)
	at org.apache.spark.rpc.netty.NettyRpcEnv.send(NettyRpcEnv.scala:186)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.send(NettyRpcEnv.scala:513)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.reviveOffers(CoarseGrainedSchedulerBackend.scala:398)
	at org.apache.spark.scheduler.TaskSchedulerImpl.executorLost(TaskSchedulerImpl.scala:503)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.disableExecutor(CoarseGrainedSchedulerBackend.scala:321)
	at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint$$anonfun$onDisconnected$1.apply(YarnSchedulerBackend.scala:200)
	at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint$$anonfun$onDisconnected$1.apply(YarnSchedulerBackend.scala:199)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint.onDisconnected(YarnSchedulerBackend.scala:199)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:143)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:211)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
16/12/08 22:34:50 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:152)
	at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:132)
	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:571)
	at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:179)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:108)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:119)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:724)
16/12/08 22:34:50 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from /10.39.7.162:49169 is closed
java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@4dbd37d9 rejected from java.util.concurrent.ThreadPoolExecutor@419daf6e[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)
	at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:109)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:228)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:228)
	at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:75)
	at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHandler.java:110)
	at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:128)
	at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:109)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:208)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:194)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)
	at io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:257)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:208)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:194)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:208)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:194)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)
	at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:182)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:208)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:194)
	at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:828)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:621)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:328)
	at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:627)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:362)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:724)
java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@62b0516c rejected from java.util.concurrent.ThreadPoolExecutor@419daf6e[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)
	at scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:109)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:228)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:228)
	at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:75)
	at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHandler.java:110)
	at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:128)
	at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:109)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:208)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:194)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)
	at io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:257)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:208)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:194)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:208)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:194)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)
	at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:182)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:208)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:194)
	at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:828)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:621)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:328)
	at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:627)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:362)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:724)
