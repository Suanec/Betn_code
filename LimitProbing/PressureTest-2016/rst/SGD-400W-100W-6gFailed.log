args : trainPath testPath iterNumorg.apache.spark.SparkConf@2ddbcdeb16/12/20 15:48:33 ERROR GPLNativeCodeLoader: Could not load native gpl library16/12/20 15:48:33 ERROR LzoCodec: Cannot load native-lzo without native-hadoop[Stage 0:======================================================>(319 + 1) / 320]                                                                                train path : /user/weibo_bigdata_ds/wulei3/shixi_enzhao/warehouse/libsvmfile/sampleData-200W-100W-903-libsvm test : /user/weibo_bigdata_ds/wulei3/shixi_enzhao/warehouse/libsvmfile/sampleData-120Y-10W-903-libsvm/test10W.libsvm iterNum : 5Training...[Stage 3:===>                                                   (18 + 40) / 320][Stage 3:======================================================>(319 + 1) / 320]                                                                                [Stage 4:================>                                      (98 + 35) / 320][Stage 4:==========================>                            (155 + 0) / 320][Stage 4:======================================================>(319 + 1) / 320]                                                                                [Stage 5:>                                                       (0 + 40) / 320][Stage 5:>                                                       (2 + 40) / 320][Stage 5:======================================================>(318 + 2) / 320][Stage 5:======================================================>(319 + 1) / 320][Stage 6:>                                                        (0 + 17) / 17][Stage 6:===>                                                     (1 + 16) / 17][Stage 6:======>                                                  (2 + 15) / 17]16/12/20 15:53:44 ERROR YarnScheduler: Lost executor 6 on 10.39.5.112: Container marked as failed: container_1470311300058_9517243_01_000005 on host: 10.39.5.112. Exit status: 143. Diagnostics: Container [pid=37459,containerID=container_1470311300058_9517243_01_000005] is running beyond physical memory limits. Current usage: 4.6 GB of 4.5 GB physical memory used; 6.3 GB of 9.4 GB virtual memory used. Killing container.Dump of the process-tree for container_1470311300058_9517243_01_000005 :	|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE	|- 37462 37459 37459 37459 (java) 81758 7422 6692147200 1199172 /usr/local/jdk1.7.0_67/bin/java -server -Xmx4096m -Djava.io.tmpdir=/data6/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000005/tmp -Dspark.driver.port=46375 -Dspark.yarn.app.container.log.dir=/data11/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000005 -XX:MaxPermSize=256m -XX:OnOutOfMemoryError=kill %p org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.39.6.238:46375 --executor-id 6 --hostname 10.39.5.112 --cores 4 --app-id application_1470311300058_9517243 --user-class-path file:/data6/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000005/__app__.jar 	|- 37459 37457 37459 37459 (bash) 0 2 108703744 313 /bin/bash -c /usr/local/jdk1.7.0_67/bin/java -server -Xmx4096m -Djava.io.tmpdir=/data6/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000005/tmp '-Dspark.driver.port=46375' -Dspark.yarn.app.container.log.dir=/data11/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000005 -XX:MaxPermSize=256m -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.39.6.238:46375 --executor-id 6 --hostname 10.39.5.112 --cores 4 --app-id application_1470311300058_9517243 --user-class-path file:/data6/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000005/__app__.jar 1> /data11/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000005/stdout 2> /data11/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000005/stderr Container killed on request. Exit code is 143Container exited with a non-zero exit code 143[Stage 6:==========>                                              (3 + 10) / 17][Stage 6:==========>                                              (3 + 14) / 17][Stage 6:=============>                                           (4 + 13) / 17][Stage 6:================>                                        (5 + 12) / 17][Stage 6:====================>                                    (6 + 11) / 17][Stage 6:=======================>                                 (7 + 10) / 17][Stage 6:=======================>                                  (7 + 9) / 17][Stage 5:>                                                        (0 + 32) / 36][Stage 5:>                                                        (0 + 36) / 36][Stage 5:=>                                                       (1 + 35) / 36][Stage 5:====>                                                    (3 + 33) / 36][Stage 5:=======>                                                 (5 + 31) / 36][Stage 5:===========>                                             (7 + 29) / 36][Stage 5:============>                                            (8 + 28) / 36][Stage 5:==============>                                          (9 + 27) / 36][Stage 5:===============>                                        (10 + 26) / 36][Stage 5:=================>                                      (11 + 25) / 36][Stage 5:=====================>                                  (14 + 22) / 36][Stage 5:========================>                               (16 + 20) / 36][Stage 5:============================>                           (18 + 18) / 36][Stage 5:==================================>                     (22 + 14) / 36][Stage 5:=====================================>                  (24 + 12) / 36][Stage 5:======================================>                 (25 + 11) / 36][Stage 5:========================================>               (26 + 10) / 36][Stage 5:============================================>            (28 + 8) / 36][Stage 5:=============================================>           (29 + 7) / 36][Stage 5:===============================================>         (30 + 6) / 36][Stage 5:==================================================>      (32 + 4) / 36][Stage 5:====================================================>    (33 + 3) / 36][Stage 5:=====================================================>   (34 + 2) / 36][Stage 5:=======================================================> (35 + 1) / 36][Stage 6:>                                                        (0 + 10) / 10][Stage 6:=====>                                                    (1 + 9) / 10][Stage 6:===========>                                              (2 + 8) / 10][Stage 6:=======================>                                  (4 + 6) / 10][Stage 6:=============================>                            (5 + 5) / 10][Stage 6:==================================>                       (6 + 4) / 10][Stage 6:========================================>                 (7 + 3) / 10][Stage 6:==============================================>           (8 + 2) / 10][Stage 6:====================================================>     (9 + 1) / 10]                                                                                [Stage 7:>                                                       (0 + 39) / 320][Stage 7:>                                                       (3 + 39) / 320][Stage 7:>                                                       (5 + 38) / 320][Stage 7:==>                                                    (15 + 36) / 320][Stage 7:===============================================>      (284 + 36) / 320][Stage 7:================================================>     (285 + 35) / 320]16/12/20 15:55:24 ERROR YarnScheduler: Lost executor 9 on 10.39.6.81: Container marked as failed: container_1470311300058_9517243_01_000011 on host: 10.39.6.81. Exit status: 143. Diagnostics: Container [pid=6527,containerID=container_1470311300058_9517243_01_000011] is running beyond physical memory limits. Current usage: 4.5 GB of 4.5 GB physical memory used; 6.2 GB of 9.4 GB virtual memory used. Killing container.Dump of the process-tree for container_1470311300058_9517243_01_000011 :	|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE	|- 6530 6527 6527 6527 (java) 104177 7813 6564868096 1183241 /usr/local/jdk1.7.0_67/bin/java -server -Xmx4096m -Djava.io.tmpdir=/data0/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000011/tmp -Dspark.driver.port=46375 -Dspark.yarn.app.container.log.dir=/data5/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000011 -XX:MaxPermSize=256m -XX:OnOutOfMemoryError=kill %p org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.39.6.238:46375 --executor-id 9 --hostname 10.39.6.81 --cores 4 --app-id application_1470311300058_9517243 --user-class-path file:/data0/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000011/__app__.jar 	|- 6527 6525 6527 6527 (bash) 0 0 108703744 313 /bin/bash -c /usr/local/jdk1.7.0_67/bin/java -server -Xmx4096m -Djava.io.tmpdir=/data0/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000011/tmp '-Dspark.driver.port=46375' -Dspark.yarn.app.container.log.dir=/data5/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000011 -XX:MaxPermSize=256m -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.39.6.238:46375 --executor-id 9 --hostname 10.39.6.81 --cores 4 --app-id application_1470311300058_9517243 --user-class-path file:/data0/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000011/__app__.jar 1> /data5/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000011/stdout 2> /data5/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000011/stderr Container killed on request. Exit code is 143Container exited with a non-zero exit code 143[Stage 7:================================================>     (285 + -6) / 320][Stage 7:================================================>     (287 + -1) / 320][Stage 7:================================================>     (288 + -1) / 320][Stage 7:================================================>     (289 + -1) / 320][Stage 7:================================================>     (290 + -1) / 320][Stage 7:=================================================>    (292 + -1) / 320][Stage 7:=================================================>    (293 + -1) / 320][Stage 7:=================================================>    (294 + -1) / 320][Stage 7:=================================================>    (295 + -1) / 320][Stage 7:==================================================>   (297 + -1) / 320][Stage 7:==================================================>   (298 + -1) / 320][Stage 7:==================================================>   (299 + -1) / 320][Stage 7:==================================================>   (300 + -1) / 320][Stage 7:==================================================>   (302 + -1) / 320][Stage 7:===================================================>  (303 + -1) / 320][Stage 7:===================================================>  (304 + -1) / 320][Stage 7:===================================================>  (305 + -1) / 320][Stage 7:===================================================>  (306 + -1) / 320][Stage 7:===================================================>  (307 + -1) / 320][Stage 7:===================================================>  (308 + -1) / 320][Stage 7:====================================================> (309 + -1) / 320][Stage 7:====================================================> (310 + -1) / 320][Stage 7:====================================================> (311 + -1) / 320][Stage 7:====================================================> (312 + -1) / 320][Stage 7:====================================================> (313 + -1) / 320][Stage 7:=====================================================>(315 + -1) / 320][Stage 7:=====================================================>(316 + -1) / 320][Stage 7:=====================================================>(317 + -1) / 320][Stage 7:=====================================================>(318 + -1) / 320][Stage 7:=====================================================>(319 + -1) / 320][Stage 7:======================================================(320 + -1) / 320][Stage 7:======================================================(322 + -2) / 320][Stage 7:======================================================(323 + -3) / 320][Stage 7:======================================================(324 + -4) / 320][Stage 7:======================================================(326 + -6) / 320][Stage 7:======================================================(327 + -7) / 320][Stage 7:======================================================(329 + -9) / 320][Stage 7:=====================================================(331 + -11) / 320][Stage 7:=====================================================(332 + -12) / 320][Stage 7:=====================================================(333 + -13) / 320][Stage 7:=====================================================(335 + -15) / 320][Stage 7:=====================================================(336 + -16) / 320][Stage 7:=====================================================(337 + -17) / 320][Stage 7:=====================================================(338 + -18) / 320][Stage 7:=====================================================(340 + -20) / 320][Stage 7:=====================================================(342 + -22) / 320][Stage 7:=====================================================(343 + -23) / 320][Stage 7:=====================================================(345 + -25) / 320][Stage 7:=====================================================(346 + -26) / 320][Stage 7:=====================================================(347 + -27) / 320][Stage 7:=====================================================(348 + -28) / 320][Stage 7:=====================================================(349 + -29) / 320][Stage 7:=====================================================(351 + -31) / 320][Stage 7:=====================================================(352 + -32) / 320][Stage 7:=====================================================(353 + -33) / 320][Stage 7:=====================================================(354 + -34) / 320][Stage 7:=====================================================(355 + -35) / 320][Stage 7:=====================================================(356 + -36) / 320][Stage 8:>                                                        (0 + 17) / 17][Stage 8:===>                                                     (1 + 16) / 17][Stage 8:======>                                                  (2 + 15) / 17][Stage 8:==========>                                              (3 + 14) / 17]16/12/20 15:55:55 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from /10.39.6.173:44615 is closed16/12/20 15:55:56 ERROR YarnScheduler: Lost executor 10 on 10.39.1.220: Container marked as failed: container_1470311300058_9517243_01_000010 on host: 10.39.1.220. Exit status: 143. Diagnostics: Container [pid=374,containerID=container_1470311300058_9517243_01_000010] is running beyond physical memory limits. Current usage: 4.5 GB of 4.5 GB physical memory used; 5.3 GB of 9.4 GB virtual memory used. Killing container.Dump of the process-tree for container_1470311300058_9517243_01_000010 :	|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE	|- 374 371 374 374 (bash) 0 0 108650496 309 /bin/bash -c /usr/local/jdk1.7.0_67/bin/java -server -Xmx4096m -Djava.io.tmpdir=/data11/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000010/tmp '-Dspark.driver.port=46375' -Dspark.yarn.app.container.log.dir=/data10/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000010 -XX:MaxPermSize=256m -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.39.6.238:46375 --executor-id 10 --hostname 10.39.1.220 --cores 4 --app-id application_1470311300058_9517243 --user-class-path file:/data11/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000010/__app__.jar 1> /data10/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000010/stdout 2> /data10/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000010/stderr 	|- 377 374 374 374 (java) 105164 5734 5609177088 1185300 /usr/local/jdk1.7.0_67/bin/java -server -Xmx4096m -Djava.io.tmpdir=/data11/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000010/tmp -Dspark.driver.port=46375 -Dspark.yarn.app.container.log.dir=/data10/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000010 -XX:MaxPermSize=256m -XX:OnOutOfMemoryError=kill %p org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.39.6.238:46375 --executor-id 10 --hostname 10.39.1.220 --cores 4 --app-id application_1470311300058_9517243 --user-class-path file:/data11/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000010/__app__.jar Container killed on request. Exit code is 143Container exited with a non-zero exit code 14316/12/20 15:55:56 ERROR YarnScheduler: Lost executor 5 on 10.39.6.173: Container marked as failed: container_1470311300058_9517243_01_000004 on host: 10.39.6.173. Exit status: 143. Diagnostics: Container [pid=12147,containerID=container_1470311300058_9517243_01_000004] is running beyond physical memory limits. Current usage: 4.5 GB of 4.5 GB physical memory used; 5.4 GB of 9.4 GB virtual memory used. Killing container.Dump of the process-tree for container_1470311300058_9517243_01_000004 :	|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE	|- 12150 12147 12147 12147 (java) 101122 4559 5654958080 1187492 /usr/local/jdk1.7.0_67/bin/java -server -Xmx4096m -Djava.io.tmpdir=/data6/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000004/tmp -Dspark.driver.port=46375 -Dspark.yarn.app.container.log.dir=/data0/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000004 -XX:MaxPermSize=256m -XX:OnOutOfMemoryError=kill %p org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.39.6.238:46375 --executor-id 5 --hostname 10.39.6.173 --cores 4 --app-id application_1470311300058_9517243 --user-class-path file:/data6/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000004/__app__.jar 	|- 12147 12145 12147 12147 (bash) 0 0 108650496 308 /bin/bash -c /usr/local/jdk1.7.0_67/bin/java -server -Xmx4096m -Djava.io.tmpdir=/data6/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000004/tmp '-Dspark.driver.port=46375' -Dspark.yarn.app.container.log.dir=/data0/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000004 -XX:MaxPermSize=256m -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.39.6.238:46375 --executor-id 5 --hostname 10.39.6.173 --cores 4 --app-id application_1470311300058_9517243 --user-class-path file:/data6/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000004/__app__.jar 1> /data0/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000004/stdout 2> /data0/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000004/stderr Container killed on request. Exit code is 143Container exited with a non-zero exit code 143[Stage 8:==========>                                              (3 + 11) / 17][Stage 8:==========>                                              (3 + 12) / 17]16/12/20 15:55:57 ERROR YarnScheduler: Lost executor 1 on 10.39.4.190: Container marked as failed: container_1470311300058_9517243_01_000014 on host: 10.39.4.190. Exit status: 143. Diagnostics: Container [pid=20616,containerID=container_1470311300058_9517243_01_000014] is running beyond physical memory limits. Current usage: 4.6 GB of 4.5 GB physical memory used; 5.5 GB of 9.4 GB virtual memory used. Killing container.Dump of the process-tree for container_1470311300058_9517243_01_000014 :	|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE	|- 20616 20614 20616 20616 (bash) 0 0 108646400 303 /bin/bash -c /usr/local/jdk1.7.0_67/bin/java -server -Xmx4096m -Djava.io.tmpdir=/data9/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000014/tmp '-Dspark.driver.port=46375' -Dspark.yarn.app.container.log.dir=/data3/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000014 -XX:MaxPermSize=256m -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.39.6.238:46375 --executor-id 1 --hostname 10.39.4.190 --cores 4 --app-id application_1470311300058_9517243 --user-class-path file:/data9/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000014/__app__.jar 1> /data3/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000014/stdout 2> /data3/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000014/stderr 	|- 20620 20616 20616 20616 (java) 108003 10254 5757796352 1211717 /usr/local/jdk1.7.0_67/bin/java -server -Xmx4096m -Djava.io.tmpdir=/data9/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000014/tmp -Dspark.driver.port=46375 -Dspark.yarn.app.container.log.dir=/data3/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000014 -XX:MaxPermSize=256m -XX:OnOutOfMemoryError=kill %p org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.39.6.238:46375 --executor-id 1 --hostname 10.39.4.190 --cores 4 --app-id application_1470311300058_9517243 --user-class-path file:/data9/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000014/__app__.jar Container killed on request. Exit code is 143Container exited with a non-zero exit code 143[Stage 8:==========>                                               (3 + 9) / 17]16/12/20 15:55:57 ERROR YarnScheduler: Lost executor 7 on 10.39.1.103: Container marked as failed: container_1470311300058_9517243_01_000008 on host: 10.39.1.103. Exit status: 143. Diagnostics: Container [pid=25722,containerID=container_1470311300058_9517243_01_000008] is running beyond physical memory limits. Current usage: 4.6 GB of 4.5 GB physical memory used; 5.4 GB of 9.4 GB virtual memory used. Killing container.Dump of the process-tree for container_1470311300058_9517243_01_000008 :	|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE	|- 25726 25722 25722 25722 (java) 110224 6803 5666803712 1193081 /usr/local/jdk1.7.0_67/bin/java -server -Xmx4096m -Djava.io.tmpdir=/data2/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000008/tmp -Dspark.driver.port=46375 -Dspark.yarn.app.container.log.dir=/data7/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000008 -XX:MaxPermSize=256m -XX:OnOutOfMemoryError=kill %p org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.39.6.238:46375 --executor-id 7 --hostname 10.39.1.103 --cores 4 --app-id application_1470311300058_9517243 --user-class-path file:/data2/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000008/__app__.jar 	|- 25722 25719 25722 25722 (bash) 0 0 108650496 309 /bin/bash -c /usr/local/jdk1.7.0_67/bin/java -server -Xmx4096m -Djava.io.tmpdir=/data2/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000008/tmp '-Dspark.driver.port=46375' -Dspark.yarn.app.container.log.dir=/data7/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000008 -XX:MaxPermSize=256m -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.39.6.238:46375 --executor-id 7 --hostname 10.39.1.103 --cores 4 --app-id application_1470311300058_9517243 --user-class-path file:/data2/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000008/__app__.jar 1> /data7/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000008/stdout 2> /data7/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000008/stderr Container killed on request. Exit code is 143Container exited with a non-zero exit code 143[Stage 8:==========>                                               (3 + 8) / 17]16/12/20 15:55:58 ERROR YarnScheduler: Lost executor 8 on 10.39.4.244: Container marked as failed: container_1470311300058_9517243_01_000009 on host: 10.39.4.244. Exit status: 143. Diagnostics: Container [pid=27059,containerID=container_1470311300058_9517243_01_000009] is running beyond physical memory limits. Current usage: 4.6 GB of 4.5 GB physical memory used; 6.3 GB of 9.4 GB virtual memory used. Killing container.Dump of the process-tree for container_1470311300058_9517243_01_000009 :	|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE	|- 27072 27059 27059 27059 (java) 99753 9758 6661906432 1193017 /usr/local/jdk1.7.0_67/bin/java -server -Xmx4096m -Djava.io.tmpdir=/data9/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000009/tmp -Dspark.driver.port=46375 -Dspark.yarn.app.container.log.dir=/data8/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000009 -XX:MaxPermSize=256m -XX:OnOutOfMemoryError=kill %p org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.39.6.238:46375 --executor-id 8 --hostname 10.39.4.244 --cores 4 --app-id application_1470311300058_9517243 --user-class-path file:/data9/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000009/__app__.jar 	|- 27059 27057 27059 27059 (bash) 0 2 108699648 311 /bin/bash -c /usr/local/jdk1.7.0_67/bin/java -server -Xmx4096m -Djava.io.tmpdir=/data9/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000009/tmp '-Dspark.driver.port=46375' -Dspark.yarn.app.container.log.dir=/data8/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000009 -XX:MaxPermSize=256m -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.39.6.238:46375 --executor-id 8 --hostname 10.39.4.244 --cores 4 --app-id application_1470311300058_9517243 --user-class-path file:/data9/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000009/__app__.jar 1> /data8/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000009/stdout 2> /data8/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000009/stderr Container killed on request. Exit code is 143Container exited with a non-zero exit code 143[Stage 7:>                                                        (0 + 1) / 219]16/12/20 15:56:00 ERROR RetryingBlockFetcher: Exception while beginning fetch of 1 outstanding blocks (after 1 retries)java.io.IOException: Failed to connect to /10.39.6.173:44615	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:228)	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:179)	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:96)	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)	at java.util.concurrent.FutureTask.run(FutureTask.java:166)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.lang.Thread.run(Thread.java:724)Caused by: java.net.ConnectException: Connection refused: /10.39.6.173:44615	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:708)	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)	... 1 more16/12/20 15:56:01 ERROR YarnScheduler: Lost executor 4 on 10.39.2.165: Container marked as failed: container_1470311300058_9517243_01_000007 on host: 10.39.2.165. Exit status: 143. Diagnostics: Container [pid=28437,containerID=container_1470311300058_9517243_01_000007] is running beyond physical memory limits. Current usage: 4.6 GB of 4.5 GB physical memory used; 5.4 GB of 9.4 GB virtual memory used. Killing container.Dump of the process-tree for container_1470311300058_9517243_01_000007 :	|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE	|- 28437 28435 28437 28437 (bash) 0 0 108650496 308 /bin/bash -c /usr/local/jdk1.7.0_67/bin/java -server -Xmx4096m -Djava.io.tmpdir=/data2/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000007/tmp '-Dspark.driver.port=46375' -Dspark.yarn.app.container.log.dir=/data0/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000007 -XX:MaxPermSize=256m -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.39.6.238:46375 --executor-id 4 --hostname 10.39.2.165 --cores 4 --app-id application_1470311300058_9517243 --user-class-path file:/data2/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000007/__app__.jar 1> /data0/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000007/stdout 2> /data0/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000007/stderr 	|- 28441 28437 28437 28437 (java) 95516 5084 5681070080 1200952 /usr/local/jdk1.7.0_67/bin/java -server -Xmx4096m -Djava.io.tmpdir=/data2/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000007/tmp -Dspark.driver.port=46375 -Dspark.yarn.app.container.log.dir=/data0/hadoop/log/yarn/application_1470311300058_9517243/container_1470311300058_9517243_01_000007 -XX:MaxPermSize=256m -XX:OnOutOfMemoryError=kill %p org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.39.6.238:46375 --executor-id 4 --hostname 10.39.2.165 --cores 4 --app-id application_1470311300058_9517243 --user-class-path file:/data2/hadoop/local/usercache/weibo_bigdata_ds/appcache/application_1470311300058_9517243/container_1470311300058_9517243_01_000007/__app__.jar Container killed on request. Exit code is 143Container exited with a non-zero exit code 143[Stage 7:>                                                        (0 + 7) / 219][Stage 7:>                                                       (0 + 15) / 219][Stage 7:>                                                       (0 + 19) / 219]16/12/20 15:56:05 ERROR RetryingBlockFetcher: Exception while beginning fetch of 1 outstanding blocks (after 2 retries)java.io.IOException: Failed to connect to /10.39.6.173:44615	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:228)	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:179)	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:96)	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)	at java.util.concurrent.FutureTask.run(FutureTask.java:166)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.lang.Thread.run(Thread.java:724)Caused by: java.net.ConnectException: Connection refused: /10.39.6.173:44615	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:708)	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)	... 1 more[Stage 7:>                                                       (1 + 19) / 219][Stage 7:>                                                       (1 + 20) / 219][Stage 7:>                                                       (2 + 20) / 219][Stage 7:>                                                       (3 + 20) / 219]16/12/20 15:56:10 ERROR RetryingBlockFetcher: Exception while beginning fetch of 1 outstanding blocks (after 3 retries)java.io.IOException: Failed to connect to /10.39.6.173:44615	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:228)	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:179)	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:96)	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)	at java.util.concurrent.FutureTask.run(FutureTask.java:166)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.lang.Thread.run(Thread.java:724)Caused by: java.net.ConnectException: Connection refused: /10.39.6.173:44615	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:708)	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)	... 1 more16/12/20 15:56:10 ERROR TaskResultGetter: Exception while getting task resultorg.apache.spark.storage.BlockFetchException: Failed to fetch block after 1 fetch failures. Most recent failure cause:	at org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:565)	at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:76)	at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:57)	at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:57)	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1857)	at org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:56)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.lang.Thread.run(Thread.java:724)Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:194)	at org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:104)	at org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:554)	... 8 moreCaused by: java.io.IOException: Failed to connect to /10.39.6.173:44615	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:228)	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:179)	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:96)	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)	at java.util.concurrent.FutureTask.run(FutureTask.java:166)	... 3 moreCaused by: java.net.ConnectException: Connection refused: /10.39.6.173:44615	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:708)	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)	... 1 moreException in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: org.apache.spark.storage.BlockFetchException: Failed to fetch block after 1 fetch failures. Most recent failure cause:	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)	at scala.Option.foreach(Option.scala:257)	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1934)	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:983)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)	at org.apache.spark.rdd.RDD.reduce(RDD.scala:965)	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1108)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1085)	at org.apache.spark.mllib.optimization.GradientDescent$.runMiniBatchSGD(GradientDescent.scala:239)	at org.apache.spark.mllib.optimization.GradientDescent.optimize(GradientDescent.scala:135)	at org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:313)	at org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:234)	at org.apache.spark.mllib.classification.LogisticRegressionWithSGD$.train(LogisticRegression.scala:293)	at org.apache.spark.mllib.classification.LogisticRegressionWithSGD$.train(LogisticRegression.scala:330)	at trainLR$.main(trainLR.scala:44)	at trainLR.main(trainLR.scala)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:606)	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729)	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)16/12/20 15:56:10 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:152)	at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:132)	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:571)	at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:179)	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:108)	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:119)	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)	at java.lang.Thread.run(Thread.java:724)16/12/20 15:56:10 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:152)	at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:132)	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:571)	at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:179)	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:108)	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:119)	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)	at java.lang.Thread.run(Thread.java:724)16/12/20 15:56:10 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:152)	at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:132)	at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:571)	at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:179)	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:108)	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:119)	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)	at java.lang.Thread.run(Thread.java:724)date start : 2016-12-20_15-47-36data size : /user/weibo_bigdata_ds/wulei3/shixi_enzhao/warehouse/libsvmfile/sampleData-1Y-100-100-libsvmiter Num : 5spark-submit --master yarn --deploy-mode client --num-executors 10 --driver-memory 6g --executor-cores 4 --executor-memory 4g --conf spark.driver.maxResultSize=3g --conf spark.ui.retainedJobs=2 --conf spark.ui.retainedStages=2 --conf spark.worker.ui.retainedExecutors=5 --conf spark.worker.ui.retainedDrivers=5 --conf spark.eventLog.enabled=false trainlrwithsgd-0-1-0_2.11-0.1.0-SNAPSHOT.jar /user/weibo_bigdata_ds/wulei3/shixi_enzhao/warehouse/libsvmfile/sampleData-200W-100W-903-libsvm /user/weibo_bigdata_ds/wulei3/shixi_enzhao/warehouse/libsvmfile/sampleData-120Y-10W-903-libsvm/test10W.libsvm 5date ended : 2016-12-20_15-56-10